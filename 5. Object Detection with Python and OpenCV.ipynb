{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong style=\"color: tomato;\">Object Detection</strong> $\\color{blue}{\\text{}}$\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">1. </span>Introduction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "- Understand various object detection methods, slowly build up to more complex methods as we go along,\n",
    "    - Template Matching\n",
    "        - Simply looking for an exavt copy of an image in another image\n",
    "    - Corner Detection\n",
    "        - Looking for corners in images\n",
    "    - Edge Detection\n",
    "        - Expanding to find general edges of objects\n",
    "    - Grid Detection\n",
    "        - Combining both concepts (corner + edge) to find grids in images (useful for applications)\n",
    "    - Contour detection\n",
    "        - Allows us to detect foreground vs background images\n",
    "        - Allow for detection of external vs internal contours (e.g. grabbing the eyes and smile from a cartoon smile face)\n",
    "    - Feature Matching\n",
    "        - More advanced methods of detecting matching objects in another image, even if the target image is not shown exactly the same in the image we are searching\n",
    "    - Watershed Algorithm\n",
    "        - Advanced algorithm that allows us to segment images into foreground and background\n",
    "        - Allows us to manually set seeds to choose segments of an image\n",
    "    - Facial and Eye Detection\n",
    "        - We will use Haar Cascades to detect facces in images\n",
    "        - This <strong style=\"font-size: 1.2em; text-transform: uppercase; color: tomato; text-decoration: underline;\">is not</strong> <strong style=\"text-transform: uppercase;\">facial recognition</strong>, that requires deep learning which will be discussed in a future section\n",
    "- Project assessment at the end of the section - we will builda full computer vision application that can blur license plates automatically"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">2. </span>Template matching."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simplest form of object detection\n",
    "- Simply scans a larger image for a provided template by sliding the template target image across the larger image\n",
    "- Main option that can be adjusted is the comparison method used as the target template is slid across the larger image\n",
    "    - The methods are all some sort of correlation based metric\n",
    "    \n",
    "<div style=\"display: flex; justify-content: center;\"><img style=\"max-width: 30%; align-self: center;\" src=\"./src/img/TM/TM_1.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = cv2.imread('../Computer-Vision-with-Python/DATA/sammy.jpg')\n",
    "full = cv2.cvtColor(full, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(full)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the template image which is subset of larger image we are searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = cv2.imread('../Computer-Vision-with-Python/DATA/sammy_face.jpg')\n",
    "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(face)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "eval function (digression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1, 2, 3]) # sum of a list\n",
    "mystring = 'sum' # it is not the function it is a string\n",
    "eval(mystring) # evaluate the string => this looks like the function \"sum\"\n",
    "myfunc = eval(mystring) # assign the sum function to the myfunc variable => we can now do myfunc([1, 2, 3]) and the result will be 6\n",
    "# eval can transform a string that matches a built in function and then actuall run that"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the 6 methods for comparison in a list\n",
    "# Note how we are using strings, later on we'll use the eval() function to convert to function\n",
    "methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR','cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in methods:\n",
    "    # create the copy of the image so that we can not accidentally mess it up\n",
    "    full_copy = full.copy()\n",
    "    method = eval(m)\n",
    "\n",
    "    # TEMPLATE MATCHING\n",
    "    res = cv2.matchTemplate(full_copy, face, method)\n",
    "    # result is basically a heatmap showing where it thinks the match is\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc # (x, y)\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "\n",
    "    height, width, chanells = face.shape\n",
    "    bottom_right = (top_left[0] + width, top_left[1] + height)\n",
    "\n",
    "    cv2.rectangle(full_copy, top_left, bottom_right, (255, 0, 0), 5)\n",
    "\n",
    "    plt.subplot(121) # give me a plot that is 1 row by 2 columns, grab the 1st one\n",
    "    plt.imshow(res)\n",
    "    plt.title('HEATMAP of Template Matching')\n",
    "\n",
    "    plt.subplot(122) # give me a plot that is 1 row by 2 columns, grab the 2nd one\n",
    "    plt.imshow(full_copy)\n",
    "    plt.title('DETECTION of Template Matching')\n",
    "    \n",
    "    # SUPER TITLE => here title everything as a method used\n",
    "    plt.suptitle(m)\n",
    "\n",
    "    # without this line the subplots will constantly overwrite eachother and at the end we will only have one plot\n",
    "    plt.show()\n",
    "\n",
    "    # print 2 new lines - ONLY FOR AESTHETICS\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">3. </span>Corner detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Part one - Harris Corner Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 1.2em; margin-left: 0em;\">What is a corner?</span>\n",
    "- A corner is a point whose local neighborhood stands in two dominant and different edge directions.\n",
    "- Simply put it is a junction of two edges, where an edge is a sudden change in image brightness.\n",
    "\n",
    "The Harris Corner Detection:\n",
    "- Was published in 1988 \n",
    "- The basic intuition is that corners can be detected by looking for significant change in all directions\n",
    "- Flat regions will have no change in all directions\n",
    "- Edges will not have a major change along the direction of the edge\n",
    "\n",
    "Shi-Tomasi Corner Detection:\n",
    "- Published in 1994\n",
    "- It made a small modification to the Harris Corner Detection which ended up with better results\n",
    "- It changes the scoring function selection criteria that Harris uses\n",
    "    - Harris: $$R = \\lambda_1\\lambda_2 - k(\\lambda_1 + \\lambda_2)$$\n",
    "    - Shi-Tomasi: $$R = min(\\lambda_1 + \\lambda_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chess = cv2.imread('../Computer-Vision-with-Python/DATA/flat_chessboard.png')\n",
    "flat_chess = cv2.cvtColor(flat_chess, cv2.COLOR_BGR2RGB)\n",
    "gray_flat_chess = cv2.imread('../Computer-Vision-with-Python/DATA/flat_chessboard.png', 0)\n",
    "real_chess = cv2.imread('../Computer-Vision-with-Python/DATA/real_chessboard.jpg')\n",
    "real_chess = cv2.cvtColor(real_chess, cv2.COLOR_BGR2RGB)\n",
    "gray_real_chess = cv2.imread('../Computer-Vision-with-Python/DATA/real_chessboard.jpg', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = np.float32(gray_flat_chess)\n",
    "# src; blockSize - neighborhood size; kernel size; k - Harris detector free parameter\n",
    "dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "\n",
    "# SHOWING PURPOSES ONLY:\n",
    "dst = cv2.dilate(dst, None) # has nothing to do with corner detection it is just to show the results\n",
    "flat_chess[dst > 0.01*dst.max()] = [255, 0, 0] # RGB; wherever the result of the cornerHarris image is greater than the 10% of its max value in the original image => red\n",
    "plt.imshow(flat_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = np.float32(gray_real_chess)\n",
    "# src; blockSize - neighborhood size; kernel size; k - Harris detector free parameter\n",
    "dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "\n",
    "# SHOWING PURPOSES ONLY:\n",
    "dst = cv2.dilate(dst, None) # has nothing to do with corner detection it is just to show the results\n",
    "real_chess[dst > 0.01*dst.max()] = [255, 0, 0] # RGB; wherever the result of the cornerHarris image is greater than the 10% of its max value in the original image => red\n",
    "plt.imshow(real_chess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Part two - Shi-Tomasi Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chess = cv2.imread('../Computer-Vision-with-Python/DATA/flat_chessboard.png')\n",
    "flat_chess = cv2.cvtColor(flat_chess, cv2.COLOR_BGR2RGB)\n",
    "gray_flat_chess = cv2.imread('../Computer-Vision-with-Python/DATA/flat_chessboard.png', 0)\n",
    "real_chess = cv2.imread('../Computer-Vision-with-Python/DATA/real_chessboard.jpg')\n",
    "real_chess = cv2.cvtColor(real_chess, cv2.COLOR_BGR2RGB)\n",
    "gray_real_chess = cv2.imread('../Computer-Vision-with-Python/DATA/real_chessboard.jpg', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src; max no of corners we want; corner quality parameter; min distance\n",
    "corners = cv2.goodFeaturesToTrack(gray_flat_chess, 64, 0.01, 10)\n",
    "\n",
    "corners = np.int0(corners)\n",
    "for i in corners:\n",
    "    x, y = i.ravel()\n",
    "    cv2.circle(flat_chess, (x, y), 3, (255, 0, 0), -1)\n",
    "\n",
    "plt.imshow(flat_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = cv2.goodFeaturesToTrack(gray_real_chess, 80, 0.01, 10)\n",
    "\n",
    "corners = np.int0(corners)\n",
    "for i in corners:\n",
    "    x, y = i.ravel()\n",
    "    cv2.circle(real_chess, (x, y), 3, (255, 0, 0), -1)\n",
    "plt.imshow(real_chess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">4. </span>Edge detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canny edge detector\n",
    "- one of the most popular edge detection algorithms \n",
    "- it is a multi-stage algorithm\n",
    "\n",
    "The process:\n",
    "- apply Gaussian filter to smooth the image in order to remove the noise\n",
    "- find the intensity gradients of the image\n",
    "- apply non-maximum suppression to get rid of spurious response to edge detection\n",
    "- apply double threshold to determine potential edges\n",
    "- track edge by hysteresis: finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges (filtration process)\n",
    "\n",
    "For high resolution images where you only want general edges, it is usually a good idea to apply a custom blur before applying the Canny Algorithm.\n",
    "\n",
    "The Canny Algorithm also requires a user to decide on low and high threshold values. We will have an equation for picking a good starting point for threshold values, but often we will need to adjust it to our particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../Computer-Vision-with-Python/DATA/sammy_face.jpg')\n",
    "plt.imshow(img)\n",
    "# we do not care about the wrong channel order in edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src; threshold 1; threshold 2\n",
    "# edges = cv2.Canny(image=img, threshold1=127, threshold2=127)\n",
    "edges = cv2.Canny(image=img, threshold1=0, threshold2=255)\n",
    "plt.imshow(edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula to calculate a good threshold (works really well after applying the blur):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median value\n",
    "med_val = np.median(img)\n",
    "\n",
    "# lower & upper bound threshold\n",
    "lower = int(max(0, .7*med_val)) # thresh 1 = either 0 or .7 of the median value, whicheveer is greater\n",
    "upper = int(min(255, 1.3*med_val)) # thresh 2 = either 255 or 1.3 of the median value, whichever is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image=img, threshold1=lower, threshold2=upper+120) # we can now start to modify the threshold BUT it will perform better oce we blured the image\n",
    "plt.imshow(edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blurring the image to improve the quality of edge detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_img = cv2.blur(img, (7, 7))\n",
    "edges = cv2.Canny(image=blurred_img, threshold1=lower, threshold2=upper+30)\n",
    "plt.imshow(edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">5. </span>Grid detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Often cameras can create a distortion in an image, such as radial distortion and tangential distortion.\n",
    "- A good way to account for these distortions when performing operations like object tracking is to have a recognizable pattern attached to the object being tracked\n",
    "- Grid patterns are often used to calibrate cameras and track motion\n",
    "- OpenCV has built in methods for tracking grids and chessboard like patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chess = cv2.imread('../Computer-Vision-with-Python/DATA/flat_chessboard.png')\n",
    "plt.imshow(flat_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method works only for chessboard patterns. EXACTLY that style\n",
    "# src; grid size\n",
    "found, corners = cv2.findChessboardCorners(flat_chess, (7, 7)) # even though the grid is 8x8, because we will not be able to detect the outer edges anyway\n",
    "# returns a boolean whether it was found AND a list of corner coordinates\n",
    "cv2.drawChessboardCorners(flat_chess, (7, 7), corners, found) # draws on the original image\n",
    "plt.imshow(flat_chess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circle based grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = cv2.imread('../Computer-Vision-with-Python/DATA/dot_grid.png')\n",
    "# src; grid size; the method of searching\n",
    "found, corners = cv2.findCirclesGrid(dots, (10, 10), cv2.CALIB_CB_SYMMETRIC_GRID)\n",
    "if found:\n",
    "    cv2.drawChessboardCorners(dots, (10, 10), corners, found)\n",
    "    plt.imshow(dots)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">6. </span>Contour detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contours are defined as simply a curve joining all the continuous points (along the boundary), having same color or intensity\n",
    "- Contours are a useful tool for shape analysis and object detection and recognition\n",
    "- OpenCV has a built in Counter finder function that can also help us differentiate between internal and external contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**findContours()**\n",
    "\n",
    "function will return back contours in an image, and based on the RETR method called, you can get back external, internal, or both:\n",
    "\n",
    "* cv2.RETR_EXTERNAL:Only extracts external contours\n",
    "* cv2.RETR_CCOMP: Extracts both internal and external contours organized in a two-level hierarchy\n",
    "* cv2.RETR_TREE: Extracts both internal and external contours organized in a  tree graph\n",
    "* cv2.RETR_LIST: Extracts all contours without any internal/external relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../Computer-Vision-with-Python/DATA/internal_external.png', 0)\n",
    "# src; mode of finding the contours int/ext; method of finding the contours\n",
    "# this mode of finding the contours grabs both internal and external\n",
    "image, contours, hierarchy = cv2.findContours(img, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# contours - list\n",
    "# hierarchy - np array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw external contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up empty array\n",
    "external_contours = np.zeros(image.shape)\n",
    "\n",
    "# For every entry in contours\n",
    "for i in range(len(contours)):\n",
    "    # last column in the array is -1 if an external contour (no contours inside of it)\n",
    "    if hierarchy[0][i][3] ==-1:\n",
    "        # We can now draw the external contours from the list of contours\n",
    "        cv2.drawContours(external_contours, contours, i, 255, -1)\n",
    "plt.imshow(external_contours, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw internal contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty array to hold internal contours\n",
    "internal_contours = np.zeros(image.shape)\n",
    "\n",
    "# Iterate through list of contour arrays\n",
    "for i in range(len(contours)):\n",
    "    # If third column value is NOT equal to -1 than its internal\n",
    "    if hierarchy[0][i][3] != -1:\n",
    "        # Draw the Contour\n",
    "        cv2.drawContours(internal_contours, contours, i, 255, -1)\n",
    "plt.imshow(internal_contours, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">7. </span>Feature matching."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far we’ve learned a lot of technical syntax, but haven’t really been able to apply it to more complex computer vision applications. From now on we will be using all our technical knowledge and syntax skills with OpenCV to create programs that are directly applicable to realistic situations! We will begin by discussing Feature Matching.\n",
    "- We’ve already seen template matching to find objects within a larger image, but it required an **exact copy** of the image. Often that isn’t useful in a real world situation!\n",
    "- Feature matching extracts defining key features from an input image (using ideas from corner, edge, and contour detection)\n",
    "- Then using a distance calculation, finds all the matches in a secondary image. This means we are no longer required to have an exact copy of the target image!\n",
    "\n",
    "We will check out 3 methods:\n",
    "- Brute-Force Matching with ORB Descriptors\n",
    "- Brute-Force Matching with SIFT Descriptors and Ratio Test\n",
    "- FLANN based Matcher\n",
    "\n",
    "We will be testing a generic cereal image and seeing if we can find its matching box in the cereal aisle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img, cmap='gray'):\n",
    "    fig = plt.figure(figsize = (12, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reeses = cv2.imread('../Computer-Vision-with-Python/DATA/reeses_puffs.png', 0)\n",
    "# display(reeses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target image\n",
    "cereals = cv2.imread('../Computer-Vision-with-Python/DATA/many_cereals.jpg', 0)\n",
    "# display(cereals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Brute-Force Matching with ORB Descriptors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# src of image we are looking for; possible mask; \n",
    "# returns keypoints and descriptors\n",
    "kp1, des1 = orb.detectAndCompute(reeses, None)\n",
    "kp2, des2 = orb.detectAndCompute(cereals, None)\n",
    "\n",
    "# way of detecting matchings; \n",
    "bruteForce = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# where the matches occur\n",
    "matches = bruteForce.match(des1, des2)\n",
    "\n",
    "# sort matches in odrer of their distance\n",
    "# less distance means a better match\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# image, keypoints; image, keypoints; matches (here we have 265 so we should limit them if we want to display them); mask; flags\n",
    "reeses_matches = cv2.drawMatches(reeses, kp1, cereals, kp2, matches[:25], None, 2)\n",
    "\n",
    "display(reeses_matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Brute-Force Matching with SIFT Descriptors and Ratio Test:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT - Scale Invariant Feature Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a SIFT object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# keypoints and descriptors\n",
    "kp1, des1  = sift.detectAndCompute(reeses, None)\n",
    "kp2, des2  = sift.detectAndCompute(cereals, None)\n",
    "\n",
    "# compare the matches\n",
    "bruteForce = cv2.BFMatcher()\n",
    "\n",
    "# DIFFERENT HERE\n",
    "# descriptors; k - number of best matches for each descriptor from a query set => now they are pairs\n",
    "matches = bruteForce.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# ratio test to chech if the matches are close to eachother in distance\n",
    "good = [] # list of good matches\n",
    "\n",
    "# ratio test match1 < 75% match2; less distance => better match\n",
    "for match1, match2 in matches:\n",
    "    # if the distance of match1 is less than .75 match2 distance than descriptor was a good match and we keep it\n",
    "    if match1.distance < .75*match2.distance:\n",
    "        good.append([match1])\n",
    "\n",
    "# len(good) = 78\n",
    "# len(matches) = 1501\n",
    "\n",
    "# passing only the good matches\n",
    "sift_matches = cv2.drawMatchesKnn(reeses, kp1, cereals, kp2, good, None, flags=2)\n",
    "display(sift_matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">c) </span>FLANN based Matcher:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLANN - Fast Library For Approximate Nearest Neighbors\n",
    "\n",
    "It is much faster than a Brute Force methods, BUT it <strong style=\"font-size: 1.2em; text-transform: uppercase; color: tomato; text-decoration: underline;\">is not</strong> going to find the best possible matches. Instead it is going to find general good matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a SIFT object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# keypoints and descriptors\n",
    "kp1, des1  = sift.detectAndCompute(reeses, None)\n",
    "kp2, des2  = sift.detectAndCompute(cereals, None)\n",
    "\n",
    "# DEFINE THE FLANN PARAMETER (with default params)\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "# creating a dictoionary\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5) # we can try different number of trees but it will take longer to process\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "good = []\n",
    "\n",
    "for match1, match2 in matches:\n",
    "    if match1.distance < .75*match2.distance:\n",
    "        good.append([match1])\n",
    "\n",
    "# flag 0 is going to draw all the feature matches but draw lines only on the best ones (it thinks are good at least)\n",
    "# flag 2 is going to ONLY draw the lines of the best matches\n",
    "flann_matches = cv2.drawMatchesKnn(reeses, kp1, cereals, kp2, good, None, flags=0)\n",
    "display(flann_matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLANN with mask to show lines and potential matching points in specific color: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a SIFT object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# sift = cv2.SIFT_create() # does not work MAYBE I have too old version of OpenCV \n",
    "\n",
    "# keypoints and descriptors\n",
    "kp1, des1  = sift.detectAndCompute(reeses, None)\n",
    "kp2, des2  = sift.detectAndCompute(cereals, None)\n",
    "\n",
    "# DEFINE THE FLANN PARAMETER (with default params)\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "# creating a dictoionary\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5) # we can try different number of trees but it will take longer to process\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# MASK\n",
    "matchesMask = [[0, 0] for i in range(len(matches))] # [0, 0] for each item in matches that we will \"turn on\" to apply the mask\n",
    "\n",
    "for i, (match1, match2) in enumerate(matches):\n",
    "    if match1.distance < .75*match2.distance:\n",
    "        matchesMask[i] = [1, 0] # keeping track of where we have a good match\n",
    "\n",
    "# drawing parameters dictionary\n",
    "draw_params = dict(matchColor=(0, 255, 0), singlePointColor=(255, 0, 0), matchesMask=matchesMask, flags=0)\n",
    "\n",
    "flann_matches = cv2.drawMatchesKnn(reeses, kp1, cereals, kp2, matches, None, **draw_params)\n",
    "display(flann_matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">8. </span>Watershed Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In geography, a watershed is a land area that channels rainfall and snowmelt to creeks, streams, and rivers, and eventually to outflow points such as reservoirs, bays, and the ocean. These watersheds can then be segmented as topographical maps with boundaries. Metaphorically, the watershed algorithm transformation treats the image it operates upon like a topographic map, with the brightness of each point representing its height, and finds the lines that run along the tops of ridges. Any grayscale image can be viewed as a topographic surface where **high intensity** denotes peaks and hills while **low intensity** denotes valleys. \n",
    "\n",
    "The algorithm can then fill every isolated valleys (local minima) with different colored water (labels). As the “water” rises, depending on the peaks (gradients) nearby, “water” from different valleys (different segments of the image), with different colors could start to merge. To avoid this merging, the algorithm creates barriers (segment edge boundaries) in locations where “water” merges. This algorithm is especially useful for segmenting images into background and foreground in situations that are difficult for other algorithms.\n",
    "\n",
    "A common example is the use of coins next to each other on a tableLater on we will also learn how to provide our own custom “seeds” that allow us to manually start where the valleys of the watersheds go. We will draw on our own seeds to an image, and then calculate the image segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img, cmap=None):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img, cmap=cmap)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Part one - separate coins problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_coins = cv2.imread('../Computer-Vision-with-Python/DATA/pennies.jpg')\n",
    "display(sep_coins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to separate the image of these coins into 7 elements - 6 coins and a background:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 1.4em;\">THE NAIVE APPROACH:</span>\n",
    "- Median blur - We have too much detail in this image, including light, the face edges on the coins, and too much detail in the background. Let's use Median Blur Filtering to blur the image a bit, which will be useful later on when we threshold.\n",
    "- Grayscale\n",
    "- Binry threshold\n",
    "- Find the contours\n",
    "- Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_blur = cv2.medianBlur(sep_coins, ksize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_sep_coins = cv2.cvtColor(sep_blur, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, sep_thresh = cv2.threshold(gray_sep_coins, 160, 255, cv2.THRESH_BINARY_INV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, contours, hierarchy = cv2.findContours(sep_thresh.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for i in range(len(contours)):\n",
    "    if hierarchy[0][i][3] == -1:\n",
    "        cv2.drawContours(sep_coins, contours, i, (255, 0, 0), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sep_coins, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Part two - fixing the problem with watershed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<p style=\"text-align: center; font-weight: bold; font-size: 2.3em; color: #dad; text-transform: uppercase;\">\n",
    "The Watershed Algorithm STEPS:\n",
    "</p><div style=\"font-size: 1.5em; text-transform: uppercase; color: yellowgreen;\">\n",
    "\n",
    "- Step 1 - Reading the image\n",
    "- Step 2 - Apply the blur\n",
    "- Step 3 - Convert the image to grayscale\n",
    "- Step 4 - Apply Threshold on the grayscale image\n",
    "- Step 5 - Noise removal (optional)\n",
    "- Step 6 - Grab the background that we are sure of\n",
    "- Step 7 - Grab the foreground that we are sure of\n",
    "- Step 8 - Finding the unknown region\n",
    "- Step 9 - Creating the label markers of sure foreground for the algorithm\n",
    "- Step 10 - Apply the Watershed Algorithm to find markers\n",
    "- Step 11 - Draw the contours on original image\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Reading the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../Computer-Vision-with-Python/DATA/pennies.jpg')\n",
    "sep_coins = img.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Apply the blur:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup>(For extra large images like this one we have to use **HUGE** kernel sizes)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.medianBlur(img, 35)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Convert the image to grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Apply Threshold (Inverse Binary + OTSU) on the grayscale image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otsu's Method\n",
    "ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "display(thresh, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Noise removal (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3, 3), np.uint8)\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "display(opening, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6 - Grab the background that we are sure of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sure_bg = cv2.dilate(opening, kernel, iterations=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7 - Grab the foreground that we are sure of:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance transform - as pixels are further away from 0s (zeros) then their value starts getting higher.\n",
    "$$\n",
    "\\text{Distance transform:} \\\\[0.7em]\n",
    "\\begin{bmatrix}\n",
    "\\\\[-0.5em]\n",
    "&\\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0}&\\\\[0.5em]\n",
    "\n",
    "\\end{bmatrix}\n",
    "\\implies\n",
    "\\begin{bmatrix}\n",
    "\\\\[-0.5em]\n",
    "&\\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{orange}{2} & \\color{orange}{2} & \\color{orange}{2} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{orange}{2} & \\color{red}{3} & \\color{orange}{2} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{orange}{2} & \\color{orange}{2} & \\color{orange}{2} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{yellow}{1} & \\color{green}{0}&\\\\[0.3em]\n",
    "&\\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0} & \\color{green}{0}&\\\\[0.5em]\n",
    "\n",
    "\\end{bmatrix}\n",
    "\\\\[0.5em]\n",
    "\\qquad\\:\\:\\text{Binary image}\\qquad\\qquad\\qquad\\quad\\;\\:\\text{Distance transformation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src; distanceType => L2 - method of calculating the distance; 5 - maskSize\n",
    "dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "\n",
    "# creating 6 object that we are sure is the foreground (based on the dist_transform)\n",
    "ret, sure_fg = cv2.threshold(dist_transform, .7*dist_transform.max(), 255, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8 - Finding the unknown region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "display(unknown, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9 - Creating the label markers of sure foreground for the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker labelling\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers + 1\n",
    "\n",
    "# Now, mark the region of unknown with zero\n",
    "markers[unknown == 255] = 0\n",
    "\n",
    "display(markers, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10 - Apply the Watershed Algorithm to find markers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = cv2.watershed(img, markers)\n",
    "display(markers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 11 - Draw the contours on original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, contours, hierarchy = cv2.findContours(markers.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for i in range(len(contours)):\n",
    "    if hierarchy[0][i][3] == -1:\n",
    "        cv2.drawContours(sep_coins, contours, i, (255, 0, 0), 10)\n",
    "display(sep_coins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">9. </span>Custom seeds with Watershed Algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section goals:\n",
    "- Understand a variety of object detection methods\n",
    "- Slowly build up to more complex methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the image & make a copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road = cv2.imread('../Computer-Vision-with-Python/DATA/road_image.jpg')\n",
    "road_copy = np.copy(road)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty space for results to be drawn onto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_image = np.zeros(road.shape[:2], np.int32)\n",
    "segments = np.zeros(road.shape, np.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Colors for markers:](https://matplotlib.org/examples/color/colormaps_reference.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import color mappings\n",
    "from matplotlib import cm\n",
    "\n",
    "# take R G B and Aplha channels\n",
    "cm.tab10(0)\n",
    "\n",
    "# extract only R G B -> change to array -> change the range to 0:255 -> change to tuple\n",
    "tuple(np.array(cm.tab10(0)[:3]) * 255)\n",
    "\n",
    "# make the above into a function:\n",
    "def createRGB(i):\n",
    "    return tuple(np.array(cm.tab10(i)[:3]) * 255)\n",
    "\n",
    "# create a colors list\n",
    "colors = []\n",
    "for i in range(10):\n",
    "    colors.append(createRGB(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should switch the RGB channels to BGR because we are going to display it using OpenCV but I can try that later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a callback function:\n",
    "- Global variables\n",
    "- Callback function\n",
    "- While True loop to display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "n_markers = 10 # number of different markers (numbers 0-9 on the keyboard)\n",
    "current_marker = 1 # index passed to the function\n",
    "markers_updated = False # check if we updated the watershed alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback\n",
    "def mouseCallback(event, x, y, flags, param):\n",
    "    global markers_updated\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # markers passed to the watershed\n",
    "        cv2.circle(marker_image, (x, y), 10, (current_marker), -1)\n",
    "\n",
    "        # markers that user sees\n",
    "        cv2.circle(road_copy, (x, y), 10, colors[current_marker], -1)\n",
    "        markers_updated = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('Image')\n",
    "cv2.setMouseCallback('Image', mouseCallback)\n",
    "\n",
    "# while True loop\n",
    "while True:\n",
    "    # window 1 - results of the watershed\n",
    "    cv2.imshow('Watershed_Algorithm', segments)\n",
    "\n",
    "    # window 2 - user input\n",
    "    cv2.imshow('Image', road_copy)\n",
    "\n",
    "    # keyboard listener\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    # close all windows\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "    # clear all the colors when pressing the 'c'\n",
    "    elif k == ord('c'):\n",
    "        road_copy = road.copy()\n",
    "        marker_image = np.zeros(road.shape[:2], np.int32)\n",
    "        segments = np.zeros(road.shape, np.uint8)\n",
    "\n",
    "    # update color choice\n",
    "    if k > 0 and chr(k).isdigit():\n",
    "        current_marker = int(chr(k))\n",
    "        \n",
    "        # CODE TO CHECK INCASE USER IS CARELESS\n",
    "        n = int(chr(k))\n",
    "        if 1 <= n <= n_markers:\n",
    "            current_marker = n\n",
    "\n",
    "    # update the markings when we click\n",
    "    if markers_updated:\n",
    "        marker_image_copy = marker_image.copy()\n",
    "        cv2.watershed(road, marker_image_copy)\n",
    "\n",
    "        segments = np.zeros(road.shape, np.uint8)\n",
    "\n",
    "        # coloring the segments through a NumPy call\n",
    "        for color_index in range(n_markers):\n",
    "            segments[marker_image_copy == (color_index)] = colors[color_index]\n",
    "        \n",
    "        markers_updated = False\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">10. </span>Face detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Introduction to face detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola-Jones Algorithm with Haar Cascades\n",
    "\n",
    "We will explore face detection using [Haar Cascades](https://en.wikipedia.org/wiki/Haar-like_feature), which is key component of the Viola-Jones object detection framework. Keep in mind, this is **face detection NOT face recognition**. We will be able to very quickly detect if a face is in an image and locate it, however we won’t know who’s face it belongs to. We would need a really large dataset and deep learning for facial recognition.\n",
    "\n",
    "In 2001 Paul Viola and Michael Jones published their method of face detection based on the simple concept of a few key features. They also came up with the idea of pre-computing an integral image to save time on calculations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first understand the main feature types that Viola and Jones proposed. The main feature types are:\n",
    "- Edge Features (white against black; horizontal/ vertical)\n",
    "- Line Features (black with white on the side; h / v)\n",
    "- Four-rectangle Features\n",
    "\n",
    "Each feature is a single value obtained by subtracting sum of pixels under white rectangle from sum of pixels under black rectangle. Realistically, our images won’t be perfect edges or lines. These features are calculated by:\n",
    "- mean(dark region) - mean(light region)\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; \">\n",
    "    <table style=\"text-align: center;\">\n",
    "        <thead>\n",
    "            <th colspan=4 style=\"text-align: center;\">IDEAL</th>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>0</td>\n",
    "                <td>0</td>\n",
    "                <td>1</td>\n",
    "                <td>1</td>\n",
    "            <tr>\n",
    "            <tr>\n",
    "                <td>0</td>\n",
    "                <td>0</td>\n",
    "                <td>1</td>\n",
    "                <td>1</td>\n",
    "            <tr>\n",
    "            <tr>\n",
    "                <td>0</td>\n",
    "                <td>0</td>\n",
    "                <td>1</td>\n",
    "                <td>1</td>\n",
    "            <tr>\n",
    "            <tr>\n",
    "                <td>0</td>\n",
    "                <td>0</td>\n",
    "                <td>1</td>\n",
    "                <td>1</td>\n",
    "            <tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    <span style=\"margin-left: 2em; text-align: center;\"> ==> </span>\n",
    "    <table style=\"margin-left: 2em; text-align: center;\">\n",
    "        <thead>\n",
    "            <th colspan=4 style=\"text-align: center;\">REALISTIC</th>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>0</td>\n",
    "                <td>0.1</td>\n",
    "                <td>0.8</td>\n",
    "                <td>1</td>\n",
    "            <tr>\n",
    "            <tr>\n",
    "                <td>0.3</td>\n",
    "                <td>0.1</td>\n",
    "                <td>0.7</td>\n",
    "                <td>0.8</td>\n",
    "            <tr>\n",
    "            <tr>\n",
    "                <td>0.1</td>\n",
    "                <td>0.2</td>\n",
    "                <td>0.8</td>\n",
    "                <td>0.8</td>\n",
    "            <tr>\n",
    "            <tr>\n",
    "                <td>0.2</td>\n",
    "                <td>0.2</td>\n",
    "                <td>0.8</td>\n",
    "                <td>0.8</td>\n",
    "            <tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "</div\n",
    ">\n",
    "\n",
    "A perfect edge would result in a value of one. The closer the result is to 1, the better the feature.\n",
    "\n",
    "$$\n",
    "\\text{sum([0.8,\\;1,\\; 0.7 ,\\;0.8,\\; 0.8,\\; 0.8,\\; 0.8,\\; 0.8]) = 6.5} \\\\[0.3em]\n",
    "\\text{sum([0,\\; 0.1,\\; 0.3,\\; 0.1,\\; 0.1,\\; 0.2,\\; 0.2,\\; 0.2])} = 1.2 \\\\[0.3em]\n",
    "\\\\[0.3em]\n",
    "\\text{Mean} = 6.5 / 8 = 0.8125\\\\[0.3em]\n",
    "\\text{Mean} = 1.2 / 8 = 0.15\\\\[0.3em]\n",
    "\\text{Delta}= 0.8125 - 0.15 = 0.6625\\\\[0.3em]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating these sums for the entire image would be very computationally expensive. The Viola-Jones algorithm solves this by using the **integral** image. Resulting in an O(1) (order 1) running time of the algorithm.\n",
    "\n",
    "An integral image is also known as a summed area table. To sum up a subrectangle of its values; each coloured spot highlights the sum inside the rectangle of that colour.\n",
    "The algorithm also saves time by going through a **cascade** of classifiers. This means we will treat the image to a series (a cascade) of classifiers based on the simple features shown earlier. Once an image fails a classifier, we can stop attempting to detect a face. \n",
    "\n",
    "A common misconception behind face detection with this algorithm is that the algorithm slowly “scans” the entire image looking for a face. This would be very inefficient, instead we pass the cascade of classifiers:\n",
    "- First we need a front facing image of a person’s face.\n",
    "- Then turn it to grayscale.\n",
    "- Then we will begin the search for Haar Cascade Features.\n",
    "- One of the very first features searched for is an edge feature indicating eyes and cheeks.\n",
    "- If the image were to fail for the search of this feature, we can quickly say there is no face.\n",
    "- If it passed, then we search for the next feature, such as the bridge of the nose.\n",
    "- We continue through this cascade (which can be thousands of features), until the algorithm detects the face.\n",
    "\n",
    "Theoretically this approach can be used for a variety of objects or detections. For example we’ll also work with a pre-trained eye detector. The downside to this algorithm is the very large data sets needed to create your own features. However, luckily many pre-trained sets of features already exist.\n",
    "\n",
    "OpenCV comes with pre-trained xml files of various Haar Cascades. Later on in the deep learning section of the course, we will see how to create our own classification algorithms for any distinct group of images (e.g. cats vs dogs). There are pre-trained .xml files in the DATA folder.\n",
    "We will also be using a pre-trained file for your upcoming project assessment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Detecting the face, eyes and creating an app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all of the necesarry images\n",
    "nadia = cv2.imread('../Computer-Vision-with-Python/DATA/Nadia_Murad.jpg', 0)\n",
    "denis = cv2.imread('../Computer-Vision-with-Python/DATA/Denis_Mukwege.jpg', 0)\n",
    "solvay = cv2.imread('../Computer-Vision-with-Python/DATA/solvay_conference.jpg', 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the clasifier and pass the .xml classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('../Computer-Vision-with-Python/DATA/haarcascades/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFace(img):\n",
    "    face_img = img.copy()\n",
    "    \n",
    "    # it returns the objects that we can draw rectangles on => x, y, width, height of the rectangle\n",
    "    face_rects = face_cascade.detectMultiScale(face_img)\n",
    "\n",
    "    for (x, y, w, h) in face_rects:\n",
    "        cv2.rectangle(face_img, (x, y), (x + w, y + h), (255, 255, 255), 10)\n",
    "\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detectFace(solvay)\n",
    "plt.imshow(result, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm returns double faces in some cases and faces where there is no face, so we have to adjust some parameters like **scale factor** and **minimum neighbours**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustedDetectFace(img):\n",
    "    face_img = img.copy()\n",
    "    \n",
    "    # it returns the objects that we can draw rectangles on => x, y, width, height of the rectangle\n",
    "    # src; scaleFactor => how much the image size is reduced at each inage scale; minNeighbors => how many neighbors each candidate rectangle should have to retain it\n",
    "    face_rects = face_cascade.detectMultiScale(face_img, scaleFactor = 1.2, minNeighbors = 5)\n",
    "\n",
    "    for (x, y, w, h) in face_rects:\n",
    "        cv2.rectangle(face_img, (x, y), (x + w, y + h), (0, 255, 0), 10)\n",
    "\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adjustedDetectFace(solvay)\n",
    "plt.imshow(result, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eye cascade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_cascade = cv2.CascadeClassifier('../Computer-Vision-with-Python/DATA/haarcascades/haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectEyes(img):\n",
    "    face_img = img.copy()\n",
    "    \n",
    "    # it returns the objects that we can draw rectangles on => x, y, width, height of the rectangle\n",
    "    # eyes_rects = eye_cascade.detectMultiScale(face_img)\n",
    "    eyes_rects = eye_cascade.detectMultiScale(face_img, scaleFactor = 1.2, minNeighbors = 5)\n",
    "\n",
    "    for (x, y, w, h) in eyes_rects:\n",
    "        cv2.rectangle(face_img, (x, y), (x + w, y + h), (255, 255, 255), 10)\n",
    "\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detectEyes(nadia)\n",
    "plt.imshow(result, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detectEyes(denis)\n",
    "plt.imshow(result, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read(0)\n",
    "\n",
    "    frame = adjustedDetectFace(frame)\n",
    "    cv2.imshow('Face Detect', frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-cvcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
