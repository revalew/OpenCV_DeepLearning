{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong style=\"color: tomato;\">Object Tracking</strong> $\\color{blue}{\\text{}}$\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Tracking Section Goals\n",
    "- Learn basic object tracking techniques\n",
    "    - Optical Flow\n",
    "    - MeanShift and CamShift\n",
    "- Understand more advanced tracking\n",
    "    - Review Built-in Tracking APIs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">1. </span>Optical flow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s begin discussing object tracking by learning about optical flow. Optical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movement of object or camera.\n",
    "\n",
    "Optical Flow Analysis has a few assumptions:\n",
    "- The pixel intensities of an object do not change between consecutive frames.\n",
    "- Neighbouring pixels have similar motion.\n",
    "\n",
    "The optical flow methods in OpenCV will first take in a given set of points and a frame. Then it will attempt to find those points in the next frame. It is up to the user to supply the points to track.\n",
    "\n",
    "Consider the following image where we display a five frame clip of a ball moving up and towards the right. Note that given just this clip, we can not determine if the ball is moving, or if the camera moved down and to the left! Using OpenCV we pass in the previous frame, previous points and the current frame to the **Lucas-Kanade function**. The function then attempts to locate the points in the current frame.\n",
    "\n",
    "The Lucas-Kanade computes optical flow for a **sparse** feature set - meaning only the points it was told to track. But what if we wanted to track all the points in a video? We can use **Gunner Farneback’s algorithm** (also built in to OpenCV) to calculate **dense** optical flow. This **dense** optical flow will calculate flow for all points in an image. It will color them black if no flow (no movement) is detected.\n",
    "\n",
    "Check out the [resource links](https://en.wikipedia.org/wiki/Optical_flow) for full descriptions and publication links for these two algorithms!\n",
    "- Note: Requires strong linear algebra skills to understand the math behind the methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Part two - Coding Lucas-Kanade function \n",
    "\n",
    "<sup>(tracking sparse points - pick few points aand track the m throughout the video):</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up parameters for Shi-Tomasi Corner Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for ShiTomasi corner detection (good features to track paper)\n",
    "corner_track_params = dict(maxCorners = 10,\n",
    "                           qualityLevel = 0.3,\n",
    "                           minDistance = 7,\n",
    "                           blockSize = 7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for Lucas Kanade Optical Flow**:\n",
    "\n",
    "Detect the motion of specific points or the aggregated motion of regions by modifying the winSize argument. This determines the integration window size. Small windows are more sensitive to noise and may miss larger motions. Large windows will “survive” an occlusion.\n",
    "\n",
    "The integration appears smoother with the larger window size.\n",
    "\n",
    "criteria has two here - the max number (10 above) of iterations and epsilon (0.03 above). More iterations means a more exhaustive search, and a smaller epsilon finishes earlier. These are primarily useful in exchanging speed vs accuracy, but mainly stay the same.\n",
    "\n",
    "When maxLevel is 0, it is the same algorithm without using pyramids (ie, calcOpticalFlowLK). Pyramids allow finding optical flow at various resolutions of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Lucas-Kanade parameters dictionary\n",
    "lk_params = dict(winSize=(200, 200), # large window => sensitive to noise; small => can capture small motions\n",
    "                 maxLevel=2, # level 2 => 1/4 resolution\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)) # we are providing 2 criteria: max num of iterations (criteria count) = 10; epsilon = 0.03 smaller epsilon => finnish earlier; they regulate speed vs accuracy of tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the image from the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Grab the very first frame of the stream\n",
    "ret, previous_frame = cap.read()\n",
    "prev_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# which points we want to track using a goodFeaturesToTrack and the corner[...] dictionary\n",
    "prevPts = cv2.goodFeaturesToTrack(prev_gray, mask=None, **corner_track_params)\n",
    "\n",
    "# mask for displaying the actual points and drawing the lines, just for visualisation\n",
    "mask = np.zeros_like(previous_frame) # create the array of zeros with the same size as the given image\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # calculate the optical flow on the grayscale frame\n",
    "    # previous frame; current frame; points from the previous frame that we want to find in the next frame; None nextPts, but we want to find them so we can not pass them; lk params\n",
    "    nextPts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, prevPts, None, **lk_params)\n",
    "\n",
    "    # use the returned status array; it outputs a status vector with each element set to 1 if the flow of the corresponding features has been found otherwise it is set to 0\n",
    "    good_new = nextPts[status == 1]\n",
    "    good_prev = prevPts[status == 1]\n",
    "\n",
    "    # Use ravel to get points to draw lines and circles\n",
    "    for i, (new, prev) in enumerate(zip(good_new, good_prev)):\n",
    "        # NumPy method \n",
    "        x_new, y_new = new.ravel()\n",
    "        x_prev, y_prev = prev.ravel()\n",
    "\n",
    "        mask = cv2.line(mask, (x_new, y_new), (x_prev, y_prev), (0, 255, 0), 3)\n",
    "\n",
    "        frame = cv2.circle(frame, (x_new, y_new), 8, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the image along with the mask we drew the line on.\n",
    "    img = cv2.add(frame, mask)\n",
    "    cv2.imshow('tracking', img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "    # Now update the previous frame and previous points\n",
    "    prev_gray = frame_gray.copy()\n",
    "    prevPts = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">c) </span>Part three - dense optical flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we have a flow object containing vector flow cartesian information. We want to convert this into polar coordinates to magnitude and angle. Currently we have a flow object containing vector flow information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags) -> flow\n",
    "\n",
    "This function computes a dense optical flow using the Gunnar Farneback's algorithm.\n",
    "\n",
    "Here are the parameters for the function and what they represent:\n",
    "* prev first 8-bit single-channel input image.\n",
    "* next second input image of the same size and the same type as prev.\n",
    "* flow computed flow image that has the same size as prev and type CV_32FC2.\n",
    "* pyr_scale parameter, specifying the image scale (\\<1) to build pyramids for each image\n",
    "    * pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous one.\n",
    "    \n",
    "* levels number of pyramid layers including the initial image; levels=1 means that no extra layers are created and only the original images are used.\n",
    "* winsize averaging window size\n",
    "    * larger values increase the algorithm robustness to image\n",
    "* noise and give more chances for fast motion detection, but yield more blurred motion field.\n",
    "* iterations number of iterations the algorithm does at each pyramid level.\n",
    "* poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel\n",
    "    * larger values mean that the image will be approximated with smoother surfaces, yielding more robust algorithm and more blurred motion field, typically poly_n =5 or 7.\n",
    "* poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good value would be poly_sigma=1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Grab the very first frame of the stream\n",
    "ret, frame1 = cap.read(0)\n",
    "prvsImg = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# hsv mask\n",
    "hsv_mask = np.zeros_like(frame1)\n",
    "hsv_mask[:, :, 1] = 255\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    nextImg = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvsImg, nextImg, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Color the channels based on the angle of travel\n",
    "    # Pay close attention to your video, the path of the direction of flow will determine color!\n",
    "    # x and y coordinates for every vector of every pixel in the image\n",
    "    mag, ang = cv2.cartToPolar(flow[:, :, 0], flow[:, :, 1], angleInDegrees = True)\n",
    "    hsv_mask[:, :, 0] = ang / 2\n",
    "    hsv_mask[:, :, 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Convert back to BGR to show with imshow from cv\n",
    "    bgr = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR)\n",
    "    cv2.imshow('frame', bgr)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "    # Set the Previous image as the next iamge for the loop\n",
    "    prvsImg = nextImg\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">2. </span>MeanShift and CAMShift tracking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>MeanShift:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most basic tracking methods are MeanShift and CAMShift.\n",
    "\n",
    "Let’s first describe the general MeanShift algorithm, then learn how to apply it for image tracking. Afterwards we will learn how to extend the MeanShift into CAMShift (Continuously Adaptive MeanShift)\n",
    "\n",
    "Imagine we have a set of points and we wanted to assign them into clusters. We take all our data points and stack red and blue points on them. (You can’t see the red points underneath). The direction to the closest cluster centroid is determined by where most of the points nearby are at. So each iteration each blue point will move closer to where the most points are at, which is or will lead to the cluster center. The red and blue datapoints overlap completely in the first iteration before the Meanshift algorithm starts. At the end of iteration 1, all the blue points move towards the clusters. Here it appears there will be either 3 or 4 clusters. The bottom clusters have begun to reach convergence. MeanShift found 3 clusters by the third iteration. After subsequent iterations, the cluster means have stopped moving. All clusters have converged and there is no more movement.\n",
    "\n",
    "It won’t always detect what may be more “reasonable”. It may have been more reasonable to detect 4 clusters in the previous situation. MeanShift can be given a target to track, calculate the color histogram of the target area, and then keep sliding the tracking window to the closest match (the cluster center).\n",
    "\n",
    "Just using MeanShift won’t change the window size if the target moves away or towards the camera. We can use CAMshift to update the size of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# FACE TRACKING\n",
    "# object detection to grab the face location, treat it as a bunch of pixels and apply MeanShift tracking on that face\n",
    "face_cascade = cv2.CascadeClassifier('../Computer-Vision-with-Python/DATA/haarcascades/haarcascade_frontalface_default.xml')\n",
    "face_rects = face_cascade.detectMultiScale(frame)\n",
    "\n",
    "# Convert this list of a single array to a tuple of (x,y,w,h) - just tracking the 1st face\n",
    "(face_x, face_y, w, h) = tuple(face_rects[0])\n",
    "track_window = (face_x, face_y, w, h)\n",
    "\n",
    "# ROI setup\n",
    "roi = frame[face_y:face_y+h, face_x:face_x+w]\n",
    "\n",
    "# hsv color mapping\n",
    "hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# find a histogram to backproject the target on each frame in order to calculate that MeanShift\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])\n",
    "\n",
    "# normalize the histogram to array values given a min of 0 and max of 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# setting up the termination criteria - either 10 iteration or move by at least 1 pt\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # calculate the backprojection based of the roi_hist that we created \n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "\n",
    "        # Apply meanshift to get the new coordinates of the rectangle\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # draw a new rectangle on the image based of the the new updated track_window\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 5)\n",
    "\n",
    "        cv2.imshow('tracking', img2)\n",
    "\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>CAMShift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# FACE TRACKING\n",
    "# object detection to grab the face location, treat it as a bunch of pixels and apply MeanShift tracking on that face\n",
    "face_cascade = cv2.CascadeClassifier('../Computer-Vision-with-Python/DATA/haarcascades/haarcascade_frontalface_default.xml')\n",
    "face_rects = face_cascade.detectMultiScale(frame) \n",
    "\n",
    "# Convert this list of a single array to a tuple of (x,y,w,h) - just tracking the 1st face\n",
    "(face_x, face_y, w, h) = tuple(face_rects[0])\n",
    "track_window = (face_x, face_y, w, h)\n",
    "\n",
    "# ROI setup\n",
    "roi = frame[face_y:face_y+h, face_x:face_x+w]\n",
    "\n",
    "# hsv color mapping\n",
    "hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# find a histogram to backproject the target on each frame in order to calculate that MeanShift\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])\n",
    "\n",
    "# normalize the histogram to array values given a min of 0 and max of 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# setting up the termination criteria - either 10 iteration or move by at least 1 pt\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # calculate the backprojection based of the roi_hist that we created \n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "\n",
    "    ######################################################################################################\n",
    "\n",
    "        # Apply meanshift to get the new coordinates of the rectangle\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "\n",
    "        # draw a new rectangle on the image based of the points\n",
    "        pts = cv2.boxPoints(ret)\n",
    "        pts = np.int0(pts)\n",
    "\n",
    "        img2 = cv2.polylines(frame, [pts], True, (0, 0, 255), 5)\n",
    "\n",
    "    ######################################################################################################\n",
    "\n",
    "        cv2.imshow('tracking', img2)\n",
    "\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">3. </span>Tracking APIs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many Object Tracking methods. Fortunately, many have been designed as simple API calls with OpenCV. Let’s explore a few of these easy to use Object Tracking APIs.\n",
    "\n",
    "BOOSTING TRACKER:\n",
    "- Based off AdaBoost algorithm (the same underlying algorithm that the HAAR Cascade based Face Detector Used).\n",
    "- Evaluation occurs across multiple frames.\n",
    "- Pros: \n",
    "    - Very well known and studied algorithm.\n",
    "- Cons:\n",
    "    - Doesn’t know when tracking has failed.\n",
    "    - Much better techniques available!\n",
    "\n",
    "MIL TRACKER:\n",
    "- Multiple Instance Learning\n",
    "- Similar to BOOSTING, but considers a neighborhood of points around the current location to create multiple instances.\n",
    "- Check project page for more details.\n",
    "- Pros: \n",
    "    - Good performance and doesn’t drift as much as BOOSTING.\n",
    "- Cons:\n",
    "    - Failure to track an object may not be reported back.\n",
    "    - Can’t recover from full obstruction.\n",
    "\n",
    "KCF TRACKER:\n",
    "- Kernelized Correlation Filters \n",
    "- Exploits some properties of the MIL Tracker and the fact that many data points will overlap, leading to more accurate and faster tracking.\n",
    "- Pros: \n",
    "    - Better than MIL and BOOSTING.\n",
    "    - Great first choice!\n",
    "- Cons:\n",
    "    - Can not recover from full obstruction of object.\n",
    "\n",
    "TLD TRACKER:\n",
    "- Tracking, Learning, and Detection\n",
    "- The tracker follows the object from frame to frame. \n",
    "- The detector localizes all appearances that have been observed so far and corrects the tracker if necessary. \n",
    "- The learning estimates detector’s errors and updates it to avoid these errors in the future.\n",
    "- Pros: \n",
    "    - Good at tracking even with obstruction in frames.\n",
    "Tracks well under large changes in scale.\n",
    "- Cons:\n",
    "    - Can provide many false positives.\n",
    "\n",
    "MedianFlow TRACKER:\n",
    "- Internally, this tracker tracks the object in both forward and backward directions in time and measures the discrepancies between these two trajectories. \n",
    "- Pros: \n",
    "    - Very good at reporting failed tracking.\n",
    "    - Works well with predictable motion.\n",
    "- Cons:\n",
    "    - Fails under large motion (fast moving objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_tracker():\n",
    "    print(\"Welcome! What Tracker API would you like to use?\")\n",
    "    print(\"Enter 0 for BOOSTING: \")\n",
    "    print(\"Enter 1 for MIL: \")\n",
    "    print(\"Enter 2 for KCF: \")\n",
    "    print(\"Enter 3 for TLD: \")\n",
    "    print(\"Enter 4 for MEDIANFLOW: \")\n",
    "    choice = input(\"Please select your tracker: \")\n",
    "    \n",
    "    if choice == '0':\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "    if choice == '1':\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "    if choice == '2':\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "    if choice == '3':\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "    if choice == '4':\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "\n",
    "\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ask_for_tracker()\n",
    "tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(tracker).split()[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ask_for_tracker()\n",
    "tracker_name = str(tracker).split()[0][1:]\n",
    "\n",
    "# Read video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Read first frame.\n",
    "ret, frame = cap.read()\n",
    "\n",
    "\n",
    "# Special function allows us to draw on the very first frame our desired ROI\n",
    "roi = cv2.selectROI(frame, False)\n",
    "\n",
    "# Initialize tracker with first frame and bounding box\n",
    "ret = tracker.init(frame, roi)\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    # Update tracker\n",
    "    success, roi = tracker.update(frame)\n",
    "    \n",
    "    # roi variable is a tuple of 4 floats\n",
    "    # We need each value and we need them as integers\n",
    "    (x,y,w,h) = tuple(map(int,roi))\n",
    "    \n",
    "    # Draw Rectangle as Tracker moves\n",
    "    if success:\n",
    "        # Tracking success\n",
    "        p1 = (x, y)\n",
    "        p2 = (x+w, y+h)\n",
    "        cv2.rectangle(frame, p1, p2, (0,255,0), 3)\n",
    "    else :\n",
    "        # Tracking failure\n",
    "        cv2.putText(frame, \"Failure to Detect Tracking!!\", (100,200), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255),3)\n",
    "\n",
    "    # Display tracker type on frame\n",
    "    cv2.putText(frame, tracker_name, (20,400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),3);\n",
    "\n",
    "    # Display result\n",
    "    cv2.imshow(tracker_name, frame)\n",
    "\n",
    "    # Exit if ESC pressed\n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27 : \n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-cvcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
