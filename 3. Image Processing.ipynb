{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong style=\"color: tomato;\">Image Processing</strong> $\\color{blue}{\\text{}}$\n",
    "---\n",
    "<!-- <style>.title {color: tomato;font-weight: bold;}.section {color: yellowgreen;}.subsection {color: royalblue;}</style> -->\n",
    "<!-- # <span class=\"title\">Image Processing</span> $\\color{blue}{\\text{}}$ -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">1. </span>Introduction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals of the section:\n",
    "- learn various image processing operations,\n",
    "- perform operations such as: Smoothing, Blurring, Morphological Operations,\n",
    "- grab properties such as color spaces and histograms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">2. </span>Color mappings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorspaces:\n",
    "- RGB - cube model,\n",
    "- HLS (Hue, Lightnes, Saturation) - cylinder model with white as max lightnes no mater H / S,\n",
    "- HSV (Hue, Saturation, Value) - cylinder model with white as a center of the cylinder no matter H / V.\n",
    "We will not be using the HSL or HSV based color images\n",
    "\n",
    "To change the colorspace we use the cvtColor() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./Computer-Vision-with-Python/DATA/00-puppy.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">3. </span>Blending and pasting images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Blending part one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To blend images we use the addWeighted() function that uses both images and combines them. \n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;<strong>addWeighted() can only be used when the images are the same size!!</strong>\n",
    "\n",
    "The formula used to blend: $\\text{} new\\_pixel = \\alpha\\times\\text{pixel\\_1} + \\beta\\times\\text{pixel\\_2} + \\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('./Computer-Vision-with-Python/DATA/dog_backpack.png')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img2 = cv2.imread('./Computer-Vision-with-Python/DATA/watermark_no_copy.png')\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1)\n",
    "# plt.imshow(img2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blending images of the same size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the images to be the same size\n",
    "img1 = cv2.resize(img1, (1200, 1200))\n",
    "img2 = cv2.resize(img2, (1200, 1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src1, alpha, src2, beta, gamma\n",
    "blended = cv2.addWeighted(img1, .8, img2, .2, 0)\n",
    "plt.imshow(blended)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blending images of different sizes:\n",
    "\n",
    "- Overlay the small image on top of a larger image <strong style=\"font-size: 120%\">(NO BLENDING)</strong>.\n",
    "NumPy reassignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('./Computer-Vision-with-Python/DATA/dog_backpack.png')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img2 = cv2.imread('./Computer-Vision-with-Python/DATA/watermark_no_copy.png')\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.resize(img2, (600, 600))\n",
    "\n",
    "large_img = img1\n",
    "small_img = img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the coordinates where we want to insert the small image\n",
    "x_offset = 0\n",
    "y_offset = 0\n",
    "\n",
    "# where we will end slicing\n",
    "x_end = x_offset + small_img.shape[1]\n",
    "y_end = y_offset + small_img.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_img[y_offset:y_end, x_offset:x_end] = small_img\n",
    "plt.imshow(large_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Blending part two"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROI - Region Of Interest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blend together images of different sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('./Computer-Vision-with-Python/DATA/dog_backpack.png')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img2 = cv2.imread('./Computer-Vision-with-Python/DATA/watermark_no_copy.png')\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img2 = cv2.resize(img2, (600, 600))\n",
    "\n",
    "# creating ROI on larger image where we want to blend the smaller image\n",
    "y_offset = img1.shape[0] - img2.shape[0]\n",
    "x_offset = img1.shape[1] - img2.shape[1]\n",
    "\n",
    "rows, cols, channels = img2.shape\n",
    "roi = img1[y_offset:img1.shape[0], x_offset:img1.shape[1]]\n",
    "plt.imshow(roi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2gray = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "# plt.imshow(img2gray, 'gray')\n",
    "\n",
    "mask_inv = cv2.bitwise_not(img2gray) # change 1 to 0\n",
    "plt.imshow(mask_inv, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the mask to include all the color channels, because now it is a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "white_bgc = np.full(img2.shape, 255, np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgc = cv2.bitwise_or(white_bgc, white_bgc, mask=mask_inv)\n",
    "bgc.shape\n",
    "plt.imshow(bgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = cv2.bitwise_or(img2, img2, mask=mask_inv)\n",
    "plt.imshow(fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_roi = cv2.bitwise_or(roi, fg)\n",
    "plt.imshow(final_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_img = img1\n",
    "small_img = final_roi\n",
    "\n",
    "large_img[y_offset:y_offset+small_img.shape[0], x_offset:x_offset+small_img.shape[1]] = small_img\n",
    "plt.imshow(large_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">4. </span>Image thresholding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a very simple method of segmenting images into different parts. Thresholding will convert an image to consist of only two valurs - B&W (binary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thresholding in OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when passing the 0 as a second parameter we read the img in grayscale\n",
    "img = cv2.imread('./Computer-Vision-with-Python/DATA/rainbow.jpg', 0)\n",
    "plt.imshow(img, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different threshold types:\n",
    "- cv2.THRESH_BINARY - change to 0/1 (above thresh => 1, below thresh => 0),\n",
    "- cv2.THRESH_BINARY_INV - change to do 1/0,\n",
    "- cv2.THRESH_TRUNC - above the threshold => threshold, below thresh => keep the original value,\n",
    "- cv2.THRESH_TOZERO - above thresh => keep original, below => 0,\n",
    "- cv2.THRESH_TOZERO_INV - above thresh => 0, below => keep original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source image, threshold, max value expected, type of threshold\n",
    "# any values above the threshold are set to 1, and any below are set to 0\n",
    "# 127 = floor(255 / 2)\n",
    "# maxval = 255 \n",
    "ret, thresh1 = cv2.threshold(img,127, 255, cv2.THRESH_BINARY_INV)\n",
    "plt.imshow(thresh1, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./Computer-Vision-with-Python/DATA/crossword.jpg', 0)\n",
    "plt.imshow(img, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPic(img):\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPic(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to play with either the threshold typy or a thresh valur\n",
    "ret, th1 = cv2.threshold(img, 170, 255, cv2.THRESH_BINARY)\n",
    "showPic(th1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links from the lecture to further read about the adaptive thresholding:\n",
    "<div style=\"font-size: 3px\">\n",
    "Adaptive Threshold\n",
    "\n",
    "https://stackoverflow.com/questions/28763419/adaptive-threshold-parameters-confusion\n",
    "\n",
    "    @param src Source 8-bit single-channel image.\n",
    "    .   @param dst Destination image of the same size and the same type as src.\n",
    "    .   @param maxValue Non-zero value assigned to the pixels for which the condition is satisfied\n",
    "    .   @param adaptiveMethod Adaptive thresholding algorithm to use, see #AdaptiveThresholdTypes.\n",
    "    .   The #BORDER_REPLICATE | #BORDER_ISOLATED is used to process boundaries.\n",
    "    .   @param thresholdType Thresholding type that must be either #THRESH_BINARY or #THRESH_BINARY_INV,\n",
    "    .   see #ThresholdTypes.\n",
    "    .   @param blockSize Size of a pixel neighborhood that is used to calculate a threshold value for the\n",
    "    .   pixel: 3, 5, 7, and so on.\n",
    "    .   @param C Constant subtracted from the mean or weighted mean (see the details below). Normally, it\n",
    "    .   is positive but may be zero or negative as well.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUT we can also automatically adjust the threshold type (adaptive thresholding)\n",
    "# src; maxval; type of adaptive thresh; type of thresh to adapt; block size - pixel neigbourhood, has to be odd number; C constant\n",
    "# th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 5, 9)\n",
    "th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 3, 8)\n",
    "showPic(th2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended = cv2.addWeighted(th1, .6, th2, .4, 0)\n",
    "showPic(blended)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">5. </span>Blurring and Smoothing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General information:\n",
    "- Smoothing can help get rid of noise or help an applicaton focus on general details.\n",
    "- There are many methods of blurring and smoothing\n",
    "- It is often combined with edge detection\n",
    "- Edge detection can detect too many edges if the image has too high resolution without blurring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods we will be exploring:\n",
    "- Gamma Correction\n",
    "    - Gamma Correction can be applied to an image to make it appear brighter / darker depending on the Gamma value chosen\n",
    "- Kernel based filters\n",
    "    - kernels can be applied over an image to produce a variety of effects\n",
    "    - the best way to explain this is through an [interactie visualization](http://setosa.io/ev/image-kernels/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenient function 1\n",
    "def loadImg():\n",
    "    img = cv2.imread('./Computer-Vision-with-Python/DATA/bricks.jpg').astype(np.float32) / 255\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenient function 2\n",
    "def displayImg(img):\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = loadImg()\n",
    "displayImg(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 3 # gamma values over 1 => make the image darker\n",
    "gamma = 1/4 # gamma values less than 1 => make the image brighter\n",
    "result = np.power(i, gamma)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blurring using Low Pass Filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = loadImg()\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "cv2.putText(img, 'bricks', (10, 600), font, 10, (255, 0, 0), 4)\n",
    "\n",
    "displayImg(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = loadImg()\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "cv2.putText(img, 'bricks', (10, 600), font, 10, (255, 0, 0), 4)\n",
    "\n",
    "kernel = np.ones((5, 5), dtype=np.float32) / 25\n",
    "kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a 2D filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source image; desired depth => -1 means assign the input depth to the desired output (input depth = output depth); kernel\n",
    "dst = cv2.filter2D(img, -1, kernel)\n",
    "displayImg(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET\n",
    "img = loadImg()\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "cv2.putText(img, 'bricks', (10, 600), font, 10, (255, 0, 0), 4)\n",
    "print('reset')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default blur kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src; kernel size;\n",
    "blurred = cv2.blur(img, (5, 5))\n",
    "displayImg(blurred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gausian blurring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src; kernel size; sigma value => standard deviation\n",
    "blurred_gauss = cv2.GaussianBlur(img, (5, 5), 10)\n",
    "displayImg(blurred_gauss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median blurring (good for reducing image noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src; kernel size (must be square size); \n",
    "blurred_median = cv2.medianBlur(img, 5)\n",
    "displayImg(blurred_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./Computer-Vision-with-Python/DATA/sammy.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "displayImg(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_img = cv2.imread('./Computer-Vision-with-Python/DATA/sammy_noise.jpg')\n",
    "# displayImg(noise_img)\n",
    "\n",
    "corrected_noise = cv2.medianBlur(noise_img, 5)\n",
    "displayImg(corrected_noise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilateral filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce unwanted noise and keep edges fairly sharp BUT it is usually slower than other filters\n",
    "blur = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "displayImg(blur)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">6. </span>Morphological operators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These operators are sets of Kernerls that can achieve a variety of  efects like reducing noise.\n",
    "- Certain operators are very good at reducing black points on a white background (and vice versa).\n",
    "- Certain operators can also achieve an erosion and dilation effect that can add or erode from an existing image.\n",
    "    - This effect can be seen best on text data so we will practice various operators on some simple white text on black background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage():\n",
    "    blank_img = np.zeros((600, 600))\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(blank_img, text='ABCDE', org=(50, 300), fontFace=font, fontScale=5, color=(255, 255, 255), thickness=25, lineType=cv2.LINE_AA)\n",
    "    return blank_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImage(img):\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = loadImage()\n",
    "displayImage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erosion - erodes away the boundaries of foreground objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5, 5), dtype=np.uint8)\n",
    "result = cv2.erode(img, kernel, iterations=4)\n",
    "displayImage(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening -> erosion + dilation (usefull to remove background noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = loadImage()\n",
    "white_noise = np.random.randint(0, 2, (600, 600)) * 255 # make the matrix of values 255 and 0\n",
    "noise_img = img + white_noise\n",
    "displayImage(noise_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening = cv2.morphologyEx(noise_img, cv2.MORPH_OPEN, kernel)\n",
    "displayImage(opening)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foreground noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = loadImage()\n",
    "black_noise = np.random.randint(0, 2, (600, 600))\n",
    "black_noise *= -255\n",
    "black_noise_img = img + black_noise\n",
    "black_noise_img[black_noise_img == -255] = 0\n",
    "displayImage(black_noise_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing - good at getting rid of the foreground noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing = cv2.morphologyEx(black_noise_img, cv2.MORPH_CLOSE, kernel)\n",
    "displayImage(closing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological gradient (takes the difference between dilation and erosion of the image):\n",
    "\n",
    "<sup>(method of edge detection)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = loadImage()\n",
    "gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)\n",
    "displayImage(gradient)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">7. </span>Gradients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image gradient is a directional change in the intensity or color in the image.\n",
    "\n",
    "- In this lecture we will be mainly exploring basic Sobel-Feldman Operators (often called Sobel for short),\n",
    "- Later on in the course we will expand on this operator for general edge detection,\n",
    "- Gradients can be calculated in a specific direction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operator ises a 3x3 kernels which are convolved with the original image to calculate approximations of the derivatives- one for horizontal change and one for vertical.\n",
    "\n",
    "<span style=\"font-size: 10px; text-decoration: none;\">[more info](https://en.wikipedia.org/wiki/Sobel_operator)</span>\n",
    "<!-- $\\text{}_{(https://en.wikipedia.org/wiki/Sobel\\_operator)}$ -->\n",
    "$$\n",
    "\\nabla _x = \n",
    "\\begin{bmatrix}\n",
    "+1 & 0 & -1 \\\\[0.3em]\n",
    "+2 & 0 & -2 \\\\[0.3em]\n",
    "+1 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "\\times A \\text{}\\quad\\text{and}\\quad \\nabla _y = \n",
    "\\begin{bmatrix}\n",
    "+1 & +2 & +1 \\\\[0.3em]\n",
    "0 & 0 & 0 \\\\[0.3em]\n",
    "-1 & -2 & -1\n",
    "\\end{bmatrix}\n",
    "\\times A\n",
    "$$\n",
    "For our usecase we will focus on understanding the syntax of using Sobel with OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def displayImage(img):\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./Computer-Vision-with-Python/DATA/sudoku.jpg', 0) # 0 => read in grayscale\n",
    "displayImage(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Sobel operators in OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth is the precision of each pixel => CV_64F means 64 floating point precision\n",
    "# dx, dy - derivatives\n",
    "sobelx = cv2.Sobel(img, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "sobely = cv2.Sobel(img, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "# displayImage(sobelx)\n",
    "displayImage(sobely)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient using Laplacian derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = cv2.Laplacian(img, cv2.CV_64F)\n",
    "displayImage(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended = cv2.addWeighted(sobelx, 1, sobely, 1, 0)\n",
    "# displayImage(blended)\n",
    "\n",
    "ret, th1 = cv2.threshold(blended, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "displayImage(th1)\n",
    "# th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 8)\n",
    "# displayImage(th2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((4,4),np.uint8)\n",
    "gradient = cv2.morphologyEx(blended, cv2.MORPH_GRADIENT, kernel)\n",
    "displayImage(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">8. </span>Histograms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Histograms part one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Histogram is a visual representation of the distribution of a continuous feature.\n",
    "- For images we can display the frquency of values of colors.\n",
    "    - Each channel has values between 0 and 255\n",
    "    - We can plot these as 3 histograms on top of each other to see how much of each channel there is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform calculations on an image, we have to stick to the cv2's expected BGR colorspace. Because of that we should create separate var image to show it in RGB after conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_horse = cv2.imread('./Computer-Vision-with-Python/DATA/horse.jpg') # original BGR for OpenCV\n",
    "show_horse = cv2.cvtColor(dark_horse, cv2.COLOR_BGR2RGB) # converted to RGB for show\n",
    "\n",
    "rainbow = cv2.imread('./Computer-Vision-with-Python/DATA/rainbow.jpg')\n",
    "show_rainbow =cv2.cvtColor(rainbow, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "blue_bricks = cv2.imread('./Computer-Vision-with-Python/DATA/bricks.jpg')\n",
    "show_bricks = cv2.cvtColor(blue_bricks, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the image we want to calculate for as a list\n",
    "# channel 0 of BGR => B\n",
    "# optional mask parameter => we want to apply the mask only to part of the image\n",
    "# histSize => upper limit\n",
    "hist_values = cv2.calcHist([blue_bricks], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "hist_values = cv2.calcHist([dark_horse], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "plt.plot(hist_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dark_horse\n",
    "color = ('b', 'g', 'r')\n",
    "for i, col in enumerate(color):\n",
    "    histr = cv2.calcHist([img], [i], None, [256], [0, 256])\n",
    "    plt.plot(histr, color=col)\n",
    "    plt.xlim([0, 256])\n",
    "    plt.ylim([0, 50000])\n",
    "plt.title('Histogram of the image')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Histograms part two - histogram of a masked section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional topics:\n",
    "- Histograms on a masked portion of the image,\n",
    "    - We can select an ROI and only calculate the color histogram of the masked section.\n",
    "- Histogram equalization.\n",
    "    - It is a method of contrast adjustment based on image's histogram,\n",
    "    - Applying it reduces the color depth (shades, higher contrast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "rainbow = cv2.imread('./Computer-Vision-with-Python/DATA/rainbow.jpg')\n",
    "show_rainbow =cv2.cvtColor(rainbow, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = rainbow\n",
    "# :2 => only first two indices\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "mask[300:400, 100:400] = 255\n",
    "\n",
    "masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "show_masked_img = cv2.bitwise_and(show_rainbow, show_rainbow, mask=mask)\n",
    "plt.imshow(show_masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src; channel; mask; histSize; ranges\n",
    "hist_mask_values_red = cv2.calcHist([rainbow], [2], mask, [256], [0, 256])\n",
    "plt.plot(hist_mask_values_red, 'r')\n",
    "plt.title('WITH mask')\n",
    "\n",
    "hist_values_red = cv2.calcHist([rainbow], [2], None, [256], [0, 256])\n",
    "plt.plot(hist_values_red, 'r')\n",
    "plt.title('WITHOUT mask')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">c) </span>Histograms part three - histogram equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gorilla = cv2.imread('./Computer-Vision-with-Python/DATA/gorilla.jpg', 0)\n",
    "\n",
    "def display(img,cmap=None):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img,cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gorilla,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_values = cv2.calcHist([gorilla], [0], None, [256], [0, 256])\n",
    "plt.plot(hist_values)\n",
    "plt.title('Histogram GRAY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_gorilla = cv2.equalizeHist(gorilla)\n",
    "# display(eq_girilla, 'gray')\n",
    "\n",
    "hist_eq_values = cv2.calcHist([eq_gorilla], [0], None, [256], [0, 256])\n",
    "plt.plot(hist_eq_values)\n",
    "plt.title('Histogram EQUALIZED GRAY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equalize the color image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_grilla = cv2.imread('./Computer-Vision-with-Python/DATA/gorilla.jpg')\n",
    "show_color_grilla = cv2.cvtColor(color_grilla, cv2.COLOR_BGR2RGB)\n",
    "color = ('b', 'g', 'r')\n",
    "\n",
    "\n",
    "for i, col in enumerate(color):\n",
    "    hist_color_values = cv2.calcHist([color_grilla], [i], None, [256], [0, 256])\n",
    "    plt.plot(hist_color_values, col)\n",
    "plt.title('Histogram COLOR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_gorilla = cv2.cvtColor(color_grilla, cv2.COLOR_BGR2HSV)\n",
    "hsv_gorilla[:,:,2].min() # value channel min == 0\n",
    "# equalization:\n",
    "hsv_gorilla[:,:,2] = cv2.equalizeHist(hsv_gorilla[:,:,2])\n",
    "# histogram 4fun\n",
    "color = ('b', 'g', 'r')\n",
    "for i, col in enumerate(color):\n",
    "    hist_hsv_eq_values = cv2.calcHist([hsv_gorilla], [i], None, [256], [0, 256])\n",
    "    plt.plot(hist_hsv_eq_values, col)\n",
    "plt.title('Histogram EQUALIZED HSV')\n",
    "\n",
    "\n",
    "hsv_eq_gorilla = cv2.cvtColor(hsv_gorilla, cv2.COLOR_HSV2RGB)\n",
    "display(hsv_eq_gorilla) # showing the higher contrast of the colored image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">9. </span>Image processing assessment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In separate notebook:\n",
    "\n",
    "D:\\1KURSY\\Kurs Python\\OpenCV_DL\\Notebooks\\Assessments\\3. 07-Image-Processing-Assessment.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-cvcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
