{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong style=\"color: tomato;\">Deep Learning</strong> $\\color{blue}{\\text{}}$\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">1. </span>Introduction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section Topics and Goals:\n",
    "- High Level Overview of Machine Learning\n",
    "- Overview of understanding classification metrics.\n",
    "- Cover Deep Learning Basics\n",
    "- Keras Basics\n",
    "- MNIST Data Overview\n",
    "- Convolutional Neural Network Theory\n",
    "- Keras CNN\n",
    "- Deep Learning on Custom Image Files\n",
    "- Understanding YOLO v3\n",
    "- YOLO v3 with Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">2. </span>Machine Learning basics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into Deep Learning, let’s work on understanding the general machine learning process we will be using.\n",
    "\n",
    "The specific case of machine learning we will be conducting is known as supervised learning. Because different students have different backgrounds in math, we will keep the mathematics behind the machine learning algorithms light. **A great textbook** on general machine learning is **Introduction to Statistical Learning by Gareth James** as a companion book. **It’s freely available online**. Simply google search the title of the book.\n",
    "\n",
    "Machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed where to look.\n",
    "\n",
    "Where is Machine Learning used:\n",
    "- Fraud detection.\n",
    "- Web search results.\n",
    "- Real-time ads on web pages \n",
    "- Credit scoring and next-best offers.\n",
    "- Prediction of equipment failures.\n",
    "- New pricing models.\n",
    "- Network intrusion detection.\n",
    "- Recommendation Engines\n",
    "- Customer Segmentation\n",
    "- Text Sentiment Analysis\n",
    "- Predicting Customer Churn\n",
    "- Pattern and image recognition.\n",
    "- Email spam filtering.\n",
    "- Financial Modeling\n",
    "\n",
    "**Supervised learning** algorithms are trained using **labeled** examples, such as an input where the desired output is known. For example, a picture could have a category label, such as either Dog or Cat. The learning algorithm receives a set of inputs along with the corresponding correct outputs, and the algorithm learns by comparing its actual output with correct outputs to find errors. It then modifies the model accordingly. Supervised learning is commonly used in applications where historical data predicts likely future events. \n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><img style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 30%;\" src=\"./src/img/ML/ML_1.png\"></div>\n",
    "\n",
    "Steps in Supervised Learning (graph above):\n",
    "- Get your data! Customers, Sensors, etc... \n",
    "- Clean and format your data (using Keras)\n",
    "- Split the data\n",
    "    - Test data (30% of images)\n",
    "    - Training data (70% of images)\n",
    "- Apply the model on a training data and let it learn simply on this data and then we build and fit the model to that training data\n",
    "- Evaluate how well the model performed => take the test data we previously set aside and test the model on it. Essentially we try to predict on the test data where we already know the correct label. This way we can evaluate the model and see how it performed\n",
    "- Then in case we want to adjust parameters in our model, we can take it back to the training and building phase and then fit the model again and then retest and we can redo the cycle over and over again, until we're finally satisfied with the evaluation metrics that our model is performing with.\n",
    "\n",
    "Once we've done that, we can deploy our model, essentially predicting on new incoming data that was outside of our original data set.\n",
    "\n",
    "Image classification and recognition is a very common and widely applicable use of deep learning and machine learning with OpenCV and Keras.\n",
    "Let’s continue by learning about how to evaluate a classification task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">3. </span>Understanding classification metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just learned that after our machine learning process is complete, we will use performance metrics to evaluate how our model did.\n",
    "Let’s discuss classification metrics in more detail!\n",
    "\n",
    "The key classification metrics we need to understand are:\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "- F1-Score\n",
    "\n",
    "But first, we should understand the reasoning behind these metrics and how they will actually work in the real world!\n",
    "\n",
    "Typically in any classification task your model can only achieve two results:\n",
    "- Either your model was correct in its prediction.\n",
    "- Or your model was incorrect in its prediction.\n",
    "\n",
    "Fortunately incorrect vs correct expands to situations where you have multiple classes. For the purposes of explaining the metrics, let’s imagine a **binary classification** situation, where we only have two available classes. In our example, we will attempt to predict if an image is a dog or a cat. Since this is supervised learning, we will first **fit / train** a model on **training data**, then **test** the model on **testing data**. Once we have the model’s predictions from the **X_test** data, we compare it to the true **y values** (the correct labels).\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><img style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\" src=\"./src/img/ClassMetr/ClassMetr_1.png\"><img style=\"margin-top: 0.5em; margin-bottom: -0.3em; margin-left: 2em; width: 25%;\" src=\"./src/img/ClassMetr/ClassMetr_2.png\"></div>\n",
    "\n",
    "We repeat this process for all the images in our X test data. At the end we will have a count of correct matches and a count of incorrect matches. The key realization we need to make, is that **in the real world, not all incorrect or correct matches hold equal value!** Also in the real world, a single metric won’t tell the complete story! To understand all of this, let’s bring back the 4 metrics we mentioned and see how they are calculated. We could organize our predicted values compared to the real values in a **confusion matrix**.\n",
    "\n",
    "**Accuracy**:\n",
    "- Accuracy in classification problems is the **number of correct predictions** made by the model divided by the **total number of predictions**.\n",
    "- For example, if the X_test set was 100 images and our model correctly predicted 80 images, then we have 80/100.\n",
    "  - 0.8 or 80% accuracy.\n",
    "- Accuracy is useful when target classes are well balanced\n",
    "  - In our example, we would have roughly the same amount of cat images as we have dog images.\n",
    "- Accuracy is **not** a good choice with **unbalanced** classes!\n",
    "- Imagine we had 99 images of dogs and 1 image of a cat. If our model was simply a line that always predicted **dog** we would get 99% accuracy!\n",
    "  - In this situation we’ll want to understand recall and precision\n",
    "\n",
    "**Recall**:\n",
    "- Ability of a model to find all the relevant cases within a dataset. \n",
    "- The precise definition of recall is the number of true positives divided by the number of true positives plus the number of false negatives. \n",
    "\n",
    "**Precision**:\n",
    "- Ability of a classification model to identify only the relevant data points.\n",
    "- Precision is defined as the number of true positives divided by the number of true positives plus the number of false positives. \n",
    "\n",
    "**Recall and Precision tradeoffs**:\n",
    "- Often you have a trade-off between Recall and Precision.\n",
    "- While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant.\n",
    "\n",
    "**F1-Score**:\n",
    "- In cases where we want to find an optimal blend of precision and recall we can combine the two metrics using what is called the F1 score.\n",
    "- The F1 score is the harmonic mean of precision and recall taking both metrics into account in the following equation:\n",
    "$$ F_1 = 2 \\times \\frac{precision \\times recall}{precision + recall}$$\n",
    "- We use the harmonic mean instead of a simple average because it punishes extreme values. \n",
    "- A classifier with a precision of 1.0 and a recall of 0.0 has a simple average of 0.5 but an F1 score of 0. \n",
    "\n",
    "We can also view all out correctly classified versus incorrectly classified images in the form of a confusion matrix.\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><img style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\" src=\"./src/img/ClassMetr/ClassMetr_3.png\"><img style=\"margin-top: 0.5em; margin-bottom: -0.3em; margin-left: 2em; width: 25%;\" src=\"./src/img/ClassMetr/ClassMetr_4.png\"></div>\n",
    "\n",
    "The main point to remember with the confusion matrix and the various calculated metrics is that they are all fundamentally ways of comparing the predicted values versus the true values.\n",
    "What constitutes “good” metrics, will really depend on the specific situation!\n",
    "We can use a confusion matrix to evaluate our model.\n",
    "For example, imagine testing for disease.\n",
    "\n",
    "<div style=\"margin-top: 0.5em; margin-bottom: 1em; display: flex; flex-direction: row; justify-content: center; align-items: center;\">\n",
    "  <ul style=\"text-align: center; list-style-type:none;\">\n",
    "      <li>Accuracy:</li>\n",
    "      <li>Overall, how often is it correct?</li>\n",
    "      <li>(TP + TN) / total = 150/165 = 0.91</li>\n",
    "  </ul>\n",
    "  <img style=\"margin-left: 2em; display: inline-block; width: 30%;\" src=\"./src/img/ClassMetr/ClassMetr_6.png\">\n",
    "  <ul style=\"text-align: center; list-style-type:none;\">\n",
    "      <li>Misclassification Rate (Error Rate):</li>\n",
    "      <li>Overall, how often is it wrong?</li>\n",
    "      <li>(FP + FN) / total = 15/165 = 0.09</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<!-- & \\frac{FP + FN}{total} = \\frac{15}{165} = 0.09 & -->\n",
    "Still confused on the confusion matrix?\n",
    "No problem! Check out the Wikipedia page for it, it has a really good diagram with all the formulas for all the metrics.\n",
    "Throughout the training, we’ll usually just print out metrics (e.g. accuracy)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">4. </span>Introduction to Deep Learning (THEORY)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next series of lectures will cover:\n",
    "- Neurons\n",
    "- Neural Networks\n",
    "- Cost Functions\n",
    "- Gradient Descent and BackPropagation\n",
    "\n",
    "Keep in mind, Keras will do all of this work for us on the back end with just a few simple line calls, but it's important to have an intuition for what is happening behind the scenes! Keep in mind, the next series of lectures is just intuition and theory only."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Understanding a Neuron - introduction to the Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will talk about:\n",
    "- Biological Neuron\n",
    "- Perceptron Model\n",
    "- Mathematical Representation\n",
    "\n",
    "Before we launch straight into neural networks, we need to understand the individual components first, such as a single “neuron”.\n",
    "Artificial Neural Networks (ANN) actually have a basis in biology! Let’s see how we can attempt to mimic biological neurons with an artificial neuron, known as a perceptron!\n",
    "\n",
    "&ensp;\n",
    "\n",
    "Biological neuron:\n",
    "- Dendrites (inputs)\n",
    "- Body (core)\n",
    "- Axon (output)\n",
    "\n",
    "The artificial neuron also has inputs and outputs! This simple model is known as a perceptron.\n",
    "- Input 0\n",
    "- Input 1\n",
    "- Output\n",
    "\n",
    "&ensp;\n",
    "\n",
    "Simple example of how it can work:\n",
    "- We have two inputs and an output. Inputs will be values of features. Inputs are multiplied by a weight. Weights initially start off as random. Inputs are now multiplied by weights. Then these results are passed to an **activation function**. Many activation functions to choose from, we’ll cover this in more detail later! For now our activation function will be very simple...\n",
    "\t- If sum of inputs is positive return 1, if sum is negative output 0. In this case 6-4=2 so the activation function returns 1. \n",
    "There is a possible issue. What if the original inputs started off as zero? Then any weight multiplied by the input would still result in zero! We fix this by adding in a **bias term**, in this case we choose 1. So what does this look like mathematically? Let’s quickly think about how we can represent this perceptron model mathematically:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n} w_ix_i + b \\\\[0.4em]\n",
    "\\text{where:}\n",
    "\\begin{cases}\n",
    "&n& & - & \\text{number of inputs,} \\\\\n",
    "&w& & - & \\text{weight of the input,} \\\\\n",
    "&x& & - & \\text{input,} \\\\\n",
    "&b& & - & \\text{bias} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "&ensp;\n",
    "\n",
    "Once we have many perceptrons in a network we’ll see how we can easily extend this to a matrix form!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Understanding a Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve seen how a single perceptron behaves, now let’s expand this concept to the idea of a neural network. Let’s see how to connect many perceptrons together and then how to represent this mathematically.\n",
    "\n",
    "Multiple Perceptrons Network:\n",
    "- Input Layer\n",
    "    - Real values from the data\n",
    "- Hidden Layers\n",
    "    - Layers in between input and output\n",
    "    - 3 or more layers is considered a “deep network”\n",
    "- Output Layer\n",
    "    - Final estimate of the output\n",
    "\n",
    "As you go forwards through more layers, the level of abstraction increases. \n",
    "Let’s now discuss the **activation functions** in a little more detail.\n",
    "\n",
    "Previously our activation function was just  a simple function that output 0 or 1. This is a pretty dramatic function, since small changes aren’t reflected. It would be nice if we could have a more dynamic function, for example the curved line gradually rising from 0 to 1. Lucky for us, this is the \n",
    "\n",
    "&ensp;\n",
    "\n",
    "- **sigmoid function** (values from 0 to 1):\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-(x)}}\\\\[0.5em]\n",
    "$$\n",
    "Changing the activation function used can be beneficial depending on the task. Let’s discuss a few more activation functions that we’ll encounter.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "- **Hyperbolic Tangent** (values from -1 to 1): $\\tanh z \\text{, where } z = wx + b$\n",
    "$$\n",
    "\\cosh x = \\frac{e^x + e^{-x}}{2} \\\\[0.5em]\n",
    "\\sinh x = \\frac{e^x - e^{-x}}{2} \\\\[0.5em]\n",
    "\\tanh x = \\frac{\\sinh x}{\\cosh x} \n",
    "$$\n",
    "&ensp;\n",
    "\n",
    "- **Rectified Linear Unit** (**ReLU**) (values from 0 to z). This is common and actually a relatively simple function: \n",
    "$$\n",
    "max(0, z) \\text{, where } z = wx + b\n",
    "$$\n",
    "&ensp;\n",
    "\n",
    "ReLu and tanh tend to have the best performance, so we will focus on these two. Deep Learning libraries (specifically Keras which we are going to work with) have these built in for us, so we don’t need to worry about having to implement them manually.\n",
    "\n",
    "As we continue on, we’ll also talk about some more state of the art activation functions.\n",
    "Up next, we’ll discuss cost functions, which will allow us to measure how well these neurons are performing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">c) </span>Cost functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now explore how we can evaluate performance of a neuron. We can use a **cost function** to measure how far off we are from the expected value.\n",
    "\n",
    "We’ll use the following variables:\n",
    "- **y** to represent the true value\n",
    "- **a** to represent neuron’s prediction\n",
    "\n",
    "In terms of weights and bias:\n",
    "- $w x + b = z$\n",
    "- Pass z into activation function $\\sigma(z) = a$\n",
    "\n",
    "&ensp;\n",
    "\n",
    "- **Quadratic Cost**:\n",
    "$$\n",
    "C = \\sum \\frac{(y - a)^2}{n}\n",
    "$$\n",
    "We can see that larger errors are more prominent due to the squaring. Unfortunately this calculation can cause a slowdown in our learning speed.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "- **Cross Entropy** (our default / go to cost function):\n",
    "$$\n",
    "C = -\\frac{1}{n} \\sum{[\\;y\\ln a + (1 - y)\\ln(1-a)\\;]}\n",
    "$$\n",
    "This cost function allows for faster learning. The larger the difference, the faster the neuron can learn.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "We now have 2 key aspects of learning with neural networks, the neurons with their activation function and the cost function. We’re still missing a key step, actually “learning”. We need to figure out how we can use our neurons and the measurement of error (our cost function) and then attempt to correct our prediction, in other words, “learn”. Now we will briefly cover how we can do this with Gradient Descent.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">d) </span>Gradient Descent and Backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’ve dabbled in machine learning before, you may have already heard of **Gradient Descent**. Let’s quickly go over it with a high level overview.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**Gradient descent** is an optimization algorithm for finding the minimum of a function. To find a local minimum, we take steps proportional to the negative of the gradient.\n",
    "Gradient Descent (in 1 dimension), visually we can see what parameter value to choose to minimize our Cost. Finding this minimum is simple for 1 dimension, but our cases will have many more parameters, meaning we’ll need to use the built-in linear algebra that our Deep Learning library will provide. Using gradient descent we can figure out the best parameters for minimizing our cost, for example, finding the best values for the weights of the neuron inputs.\n",
    "\n",
    "We now just have one issue to solve, how can we quickly adjust the optimal parameters or weights across our entire network? This is where backpropagation comes in. \n",
    "\n",
    "**Backpropagation** is used to calculate the error contribution of each neuron after a batch of data is processed. It relies heavily on the chain rule to go back through the network and calculate these errors. Backpropagation works by calculating  the error at the output and then distributes back through the network layers. It requires a known desired output for each input value (**supervised learning**). \n",
    "\n",
    "&ensp;\n",
    "\n",
    "We now know enough terminology and theory to begin working with Keras! We’ll be using a data set of image **features** of counterfeit versus real bills.\n",
    "Later on we will learn about **convolutional neural networks** for handling image data directly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">5. </span>Keras Basics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s learn how to create a machine learning model with *Keras* library with *Tensorflow* as a backend. \n",
    "\n",
    "We’ll start with some data on currency bank notes. Some of these bank notes were forgeries and others were legitimate. The researchers created a dataset from these bank notes by taking 400x400 images of the notes and then extracting various numerical features based off the wavelets of the images.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**Very important note!**\n",
    "- The data we’re working with in this lecture **is not** an image. We’re focusing right now on how to use Keras for general machine learning. Insted we are using features that were extracted from the images.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**Our goal** now is to:\n",
    "- Know how to use Keras, \n",
    "- General syntax.\n",
    "- How to perform a machine learning task such as:\n",
    "    - Getting data,\n",
    "    - Reading it in,\n",
    "    - Splitting it into test set and training set,\n",
    "    - Fitting the model to that training data,\n",
    "    - Then predicting a new unseen data such as the test set\n",
    "\n",
    "Once we learn about Convolutional Neural Networks, then we can expand on Keras to feed in image data (pixel images) into a network. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt # generate an array fron a text file "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**:\n",
    "\n",
    "We will use the Bank Authentication Data Set to start off with. This data set consists of various image features derived from images that had 400 x 400 pixels. You should note **the data itself that we will be using ARE NOT ACTUAL IMAGES**, they are **features** of images. In the next lecture we will cover grabbing and working with image data with Keras. This notebook focuses on learning the basics of building a neural network with Keras.\n",
    "\n",
    "More info on the data set [here](https://archive.ics.uci.edu/ml/datasets/banknote+authentication).\n",
    "\n",
    "Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. variance of Wavelet Transformed image (continuous) \n",
    "2. skewness of Wavelet Transformed image (continuous) \n",
    "3. curtosis of Wavelet Transformed image (continuous) \n",
    "4. entropy of image (continuous) \n",
    "5. class (integer)\n",
    "\n",
    "&ensp;\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in the Data Set**:\n",
    "\n",
    "We already have a prepared .csv files (comma separated value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are passing the delimiter parameter to specify that the features are separated by a comma\n",
    "data = genfromtxt('../Computer-Vision-with-Python/DATA/bank_note_data.txt', delimiter=',')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that at the end of the columns we have a 0s and 1s. This indicates whether or not it was an actual authentic note. 0 => forged, 1 => real. This feature is called a **label** or a **class**. We are going to classify - or rather are going to build a machine learning model that can classify - these bank notes just based on the features, without having the label.\n",
    "\n",
    "To achieve that we first have to **separate the labels from the actual features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could make it more universal I guess, but here it does not matter\n",
    "# label_index = len(data[1, :]) - 1\n",
    "# labels = data[:, label_index]\n",
    "# features_no = len(data[1, :]) - 1\n",
    "# features = data[:, 0:features_no]\n",
    "\n",
    "labels = data[:, 4] # only class telling real / fake\n",
    "features = data[:, :4] # only features, no class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the Data into Training and Test**:\n",
    "\n",
    "Its time to split the data into a train / test set. Keep in mind, sometimes people like to split 3 ways, train/test/validation. We'll keep things simple for now.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "By convention we use a capital X for features and lowercase y for label (due to the mathematical notation used in the papers, and X is a 2D matrix and y is a singular array / vector). We can still just use the 'features' and 'labels' notation if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = labels\n",
    "\n",
    "# it is going to split the features and the labels into a train set and training set\n",
    "#  this (train_...) also does randomized shuffling so we do not havr to worry about this concern that the labels happen to be sorted order\n",
    "# -> this will automatically shuffle them for us\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# passing the X features; y labels; test size of 33%; random_state => seed to have the same shuffle every time\n",
    "# why 42? => https://news.mit.edu/2019/answer-life-universe-and-everything-sum-three-cubes-mathematics-0910\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # copied straight fron the docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing the Data**:\n",
    "\n",
    "Usually when using Neural Networks, you will get better performance when you standardize the data. Standardization just means normalizing the values to all fit between a certain range, like 0-1, or -1 to 1.\n",
    "\n",
    "The [scikit learn library](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) also provides a nice function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force all the feature data to fall within a certain range\n",
    "# this can actually help the neural network perform better\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_object = MinMaxScaler()\n",
    "\n",
    "# fit the scaler object to our training data\n",
    "# fit() finds the min and max value and then transform() is transforming the given array based on the MinMax we just calculated durring the fit\n",
    "scaler_object.fit(X_train)\n",
    "# we only fit to X_train and not X_test BECAUSE we do not want the scaler_object to peek at any test data - it would be cheating. If we would do that it is called data leakage and is essentially cheating. So we fit to the train data but transform both\n",
    "scaled_X_train = scaler_object.transform(X_train)\n",
    "scaled_X_test = scaler_object.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have the data scaled. We can move on to building the network with Keras.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**Building the Network with Keras**:\n",
    "\n",
    "Building a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# creates the model\n",
    "model = Sequential()\n",
    "\n",
    "# adding the layers\n",
    "# add the dense layer, expecting 4 features (we have 4 neurons), input dimention; activation function ReLu\n",
    "model.add(Dense(4, input_dim = 4, activation = 'relu'))\n",
    "\n",
    "# here we can play arround with the neurons; too large / too small => bad results; we can do 1x or 2x input dimensions; we do not specify the input dim as it is not the input layer - it is a hidden layer\n",
    "model.add(Dense(8, activation= 'relu'))\n",
    "\n",
    "# 1 because we only have 1 neuron which has 1 output and is outputting the result of either 0 or 1; activation type sigmoid => fit between 0 and 1\n",
    "model.add(Dense(1, activation= 'sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile Model**:\n",
    "\n",
    "After setting up the model as sequential and adding the layers, we have to **compile** the model. For the compilation process we have to choose a loss and optimizer, and the metrics that we are concerned about during fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit (Train) the Model**:\n",
    "\n",
    "epochs (iterations on a dataset) - Trains the model for a given number of epochs (iterations on a dataset). 1 epoch => 1 iteration through all the data in dataset. Depending on the speed of the PC we should pick a lower number of epochs.\n",
    "\n",
    "verbose - reporting back / printing the results as it is training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(scaled_X_train, y_train, epochs= 50, verbose= 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting New Unseen Data**:\n",
    "\n",
    "Let's see how we did by predicting on **new data**. Remember, our model has **never** seen the test data that we scaled previously. This process is the exact same process you would use on totally brand new data. For example, a brand new bank note that you just analyzed ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spits out probabilities by default.\n",
    "# model.predict(scaled_X_test)\n",
    "\n",
    "model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating Model Performance**:\n",
    "\n",
    "So how well did we do? How do we actually measure \"well\". Is 95% accuracy good enough? It all depends on the situation. Also we need to take into account things like recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "predictions = model.predict_classes(scaled_X_test)\n",
    "\n",
    "# we have the answers because we have the y_test vector\n",
    "confusion_matrix(y_test, predictions)\n",
    "\n",
    "# [True Negative, False Negative]\n",
    "# [False Positive, True Positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the metrics\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving and Loading Models**:\n",
    "\n",
    "Now that we have a model trained, we can save and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('../myTestModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from keras.models import load_model\n",
    "newmodel = load_model('../myTestModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the loaded model to predict classes\n",
    "newmodel.predict_classes(scaled_X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">6. </span>MNIST Dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic data set in Deep Learning is the **MNIST data set**. Let’s quickly cover some basics about it since we’ll be using similar data concepts quite frequently during this section.\n",
    "\n",
    "Fortunately this data is easy to access with Keras. The data set has:\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "\n",
    "&ensp;\n",
    "\n",
    "The MNIST data set contains handwritten single digits from 0 to 9. A single digit image can be represented as an array. Specifically, 28 by 28 pixels. The values represent the grayscale image. Often, when working with image data, we will normalize the data to fall in range of 0 and 1 and this data set has it already set for us. We can think of the entire group of the 60,000 images as a 4-dimensional array. 60,000 images of 1 chanel 28 by 28 pixels. \n",
    "\n",
    "This array has **4** dimensions\n",
    "- (60000, 28, 28, 1)\n",
    "- (Samples, x, y, channels)\n",
    "- For color images, the last dimension value would be 3 (RGB)\n",
    "\n",
    "&ensp;\n",
    "\n",
    "For the labels we’ll use One-Hot Encoding. This means that instead of having labels such as “One”, “Two”, etc… we’ll have a single array for each image. This means if the original labels of the images are given as a list of numbers\n",
    "- [5, 0, 4, ...., 5, 6, 8]\n",
    "- We will convert them to one-hot encoding (easily done with Keras)\n",
    "\n",
    "The label is represented  based off the index position in the label array. The corresponding label will be a 1 at the index location and zero everywhere else. For example, a drawn digit of 4 would have this label array:\n",
    "- [0, 0, 0, 0, **1**, 0, 0, 0, 0 , 0]\n",
    "\n",
    "As a result, the labels for the training data ends up being a large 2-d array (60000, 10)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">7. </span>Convolutional Neural Networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Part one - Convolutional Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a Neural Network for already defined features. But what if we have the raw image data? We need to learn about **Convolutional Neural Networks** in order to effectively solve the problems that image data can present.\n",
    "\n",
    "Just like the simple perceptron, CNNs also have their origins in biological research. Hubel and Wiesel studied the structure of the visual cortex in mammals, winning a Nobel Prize in 1981. Their research revealed that neurons in the visual cortex had a small local receptive field. This idea then inspired an ANN architecture that would become CNN. Famously implemented in the 1998 paper by Yann LeCun et al. The LeNet-5 architecture was first used to classify the MNIST data set.\n",
    "\n",
    "When learning about CNNs you’ll often see a diagram like this:\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_1.png\">\n",
    "</div></div>\n",
    "\n",
    "Let’s break down the various aspects of a CNN seen here:\n",
    "- Tensors\n",
    "- DNN vs CNN\n",
    "- Convolutions and Filters\n",
    "- Padding\n",
    "- Pooling Layers\n",
    "- Review Dropout\n",
    "\n",
    "&ensp;\n",
    "\n",
    "Recall that Tensors are N-Dimensional Arrays that we build up to:\n",
    "- Scalar - 3\n",
    "- Vector - [3, 4, 5]\n",
    "- Matrix - [ [3, 4] , [5, 6] , [7, 8] ]\n",
    "- <span style=\"color: #fff;\">Tensor - </span>\n",
    "<span style=\"color: pink;\"> [\n",
    "  <span style=\"color: yellow;\">[<span style=\"color: tomato;\"> [ 1, 2], </span><span style=\"color: green;\">[ 3, 4]</span> ],</span>\n",
    "  <br>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;\n",
    "  <span style=\"color: royalblue;\">[ [ 5, 6] , [ 7, 8] ]</span>\n",
    "]</span>\n",
    "\n",
    "<!-- - Tensor - $\\color{green}{\\text{[}}\n",
    "\\color{orange}{\\text{[\\color{red}{\\text{[ 1, 2]}}, [ 3, 4] }}],\\\\[0.3em]\n",
    "\\qquad\\quad\\;\\color{yellow}{\\text{[[ 5, 6] , [ 7, 8]]}} \\color{green}{\\text{]}}$ -->\n",
    "\n",
    "Tensors make it very convenient to feed in sets of images into our model - (I,H,W,C):\n",
    "- I : Images\n",
    "- H: Height of Image in Pixels\n",
    "- W: Width of Image in Pixels\n",
    "- C: Color Channels: Grayscale - 1 channel, RGB - 3 channels\n",
    "\n",
    "Now let’s explore the difference between a Densely Connected Neural Network and a Convolutional Neural Network. Recall that we’ve already been able to create DNNs with tf.estimator API.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><span>Densely Connected layer:</span><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_2.png\">\n",
    "</div><span>Convolutional Layer:</span><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_3.png\">\n",
    "</div></div>\n",
    "\n",
    "- In **densely connected layer** every neuron in 1 layer is direcly connected to every other neuron in the next layer.\n",
    "- In **convolutional layer** each unit is connected to a smaller number of nearby units in next layer.\n",
    "\n",
    "So why bother with a CNN instead of a DNN?\n",
    "- The MNIST dataset was 28 by 28 pixels (784 total), but most images are at least 256 by 256 or greater (<56k total, and this is a small image with resolution of just 256px!). This leads to too many parameters, unscalable to new images.\n",
    "- Convolutions also have a major advantage for image processing, where pixels nearby to each other are much more correlated to each other for image detection.\n",
    "- Each CNN layer looks at an increasingly larger part of the image. Having units only connected to nearby units also aids in *invariance*.\n",
    "- CNN also helps with regularization, limiting the search of weights to the size of the convolution.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "Let’s explore how the convolutional neural network relates to image recognition. We start with the input layer, the image itself. Convolutional layers are only connected to pixels in their respective fields.\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><span></span><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_4.png\">\n",
    "</div></div>\n",
    "\n",
    "We run into a possible issue for edge neurons. There may not be an input there for them. We can fix this by adding a “padding” of zeros around the image.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><span></span><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_5.png\">\n",
    "</div></div>\n",
    "\n",
    "&ensp;\n",
    "\n",
    "Let’s walk through 1-D Convolution in more detail, then expand this idea to 2-D Convolution. Let’s revisit our DNN and convert it to a CNN.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; margin-left: 1em; width: 25%;\"><span>A DNN:</span>\n",
    "<img style=\"\" src=\"./src/img/CNN/CNN_6.png\">\n",
    "</div><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; margin-left: 1em; width: 25%;\"><span>1-D Convolution:</span>\n",
    "<img style=\"\" src=\"./src/img/CNN/CNN_7.png\">\n",
    "</div></div>\n",
    "\n",
    "We can treat these weights as a filter (1-D Convolution).\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 \\\\[0.3em]\n",
    "\\text{if} (w_1, w_2) = (1, -1) \\text{, then: } y = x_1 + x_2 \\\\[0.3em]\n",
    "\\text{When is $y$ at a maximum?} \\\\[0.3em]\n",
    "(x_1, x_2) = (1, 0)\n",
    "$$\n",
    "\n",
    "We now have a set of weights that can act as a filter for edge detection. We can then expand this idea to multiple filters.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\"><ul><li style=\"list-style: none;\">Example:</li><li>Filters: 1</li><li>Filter Size: 2</li><li>Stride: 2 / 1 (1 unit at a time)</li></ul><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; margin-left: 2em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_8.png\">\n",
    "</div></div>\n",
    "\n",
    "In the example above we are saying that we have 1 filter, the size of the filter is 2 because we have 2 weihgts, and the stride is 2 or 1.\n",
    "\n",
    "*The stride of 2 means we are moving up these weights two neurons at a time. So when we start from the bottom we repeat the weights every two neurons along the input. That is known as a stride of this filter. When the stride of the filter is set to 1, as we can see on the image above (where we aply 4 filters), that means we repeat the weights every neuron.*\n",
    "\n",
    "Remember that:\n",
    "- We can add zero padding to include more edge pixels to finsh off that stride count.\n",
    "- Each filter is detecting a different feature. \n",
    "\n",
    "&ensp;\n",
    "\n",
    "For simplicity, we begin to describe and visualize these sets of neurons as blocks instead:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_9.png\">\n",
    "</div></div>\n",
    "\n",
    "Let’s now expand these concepts to 2-D Convolution, since we’ll mainly be dealing with images.\n",
    "We have the section of input image and that section is coresponding to the section of tensors.\n",
    "\n",
    "<div style=\"margin-bottom: 2em; display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\"><span>2D Convolution:</span>\n",
    "<img src=\"./src/img/CNN/CNN_10.png\">\n",
    "</div><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; margin-left: 2em; width: 25%;\"><span>2D Color images:</span>\n",
    "<img src=\"./src/img/CNN/CNN_11.png\">\n",
    "</div></div>\n",
    "\n",
    "<div style=\"display: flex; flex-direction: row; flex-wrap: wrap; justify-content: center; align-items: center;\">\n",
    "\n",
    "  <div style=\"display: flex; flex-direction: column; justify-content: center; text-align: center; width: 35%; height: 40%; border: 1px solid #000;\">\n",
    "    <span>Filters are commonly visualized with grids:</span>\n",
    "    <img style=\"max-width: 100%; margin: 1em; height: auto;\" src=\"./src/img/CNN/CNN_12.png\">\n",
    "  </div>\n",
    "\n",
    "  <div style=\"display: flex; flex-direction: column; justify-content: center; text-align: center; width: 35%; height: 40%; border: 1px solid #000;\">\n",
    "    <span>Stride Distance of 1 Example:</span>\n",
    "    <img style=\"max-width: 100%; margin: 1em; height: auto;\" src=\"./src/img/CNN/CNN_13.png\">\n",
    "  </div>\n",
    "\n",
    "  <div style=\"display: flex; flex-direction: column; justify-content: center; text-align: center; width: 35%; height: 40%; border: 1px solid #000;\">\n",
    "    <span>Representation of Multiple Filters:</span>\n",
    "    <img style=\" max-width: 100%; margin: 1em; height: auto;\" src=\"./src/img/CNN/CNN_14.png\">\n",
    "  </div>\n",
    "\n",
    "  <div style=\"display: flex; flex-direction: column; justify-content: center; text-align: center; width: 35%; height: 40%; border: 1px solid #000;\">\n",
    "    <span>At the original CNN diagram, exactly what we saw here?:</span>\n",
    "    <img style=\" max-width: 100%; margin: 1em; height: auto;\" src=\"./src/img/CNN/CNN_15.png\">\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Now it is time to discuss subsampling (pooling)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Part two - pooling layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand convolutional layers, let’s discuss **pooling layers**. Pooling layers will subsample the input image, which reduces the memory use and computer load as well as reducing the number of parameters. \n",
    "\n",
    "Let’s imagine a layer of pixels in our input image. For our MNIST digits set, each pixel had a value representing “darkness”. We create a 2 by 2 pool of pixels (known as a kernel) and evaluate the maximum value:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_16.png\">\n",
    "</div></div>\n",
    "\n",
    "Only the max value makes it to the next layer. We then move over by a “stride”, in this case, our stride is two.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_17.png\">\n",
    "</div></div>\n",
    "\n",
    "This pooling layer will end up removing a lot of information, even a small pooling “kernel” of 2 by 2 with a stride of 2 will remove 75% of the input data.\n",
    "\n",
    "Another common technique deployed with CNN is called “Dropout”\n",
    "- Dropout can be thought of as a form of regularization to help prevent overfitting.\n",
    "- During training, units are randomly dropped, along with their connections.\n",
    "    - This helps prevent units from “co-adapting” too much.\n",
    "\n",
    "Let’s also quickly point out some famous CNN architectures:\n",
    "- LeNet-5 by Yann LeCun\n",
    "- AlexNet by Alex Krizhevsky et al.\n",
    "- GoogLeNet by Szegedy at Google Research\n",
    "- ResNet by Kaiming He et al.\n",
    "- Check out the resource links to the papers discussing these architectures!\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_18.png\">\n",
    "</div><span style=\"margin: 1em;\">AlexNet</span><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/CNN/CNN_19.png\">\n",
    "</div></div>\n",
    "\n",
    "&ensp;\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\"><span>Convolutional Neural Network</span>\n",
    "<img src=\"./src/img/CNN/CNN_1.png\">\n",
    "</div></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">8. </span>Keras CNN with MNIST dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN - Convolutional Neural Networks (in case someone does not remember)\n",
    "\n",
    "**Convolutional Neural Networks for Image Classification**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# load the mnist data \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the Image Data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_train.shape # does not have the color channel\n",
    "\n",
    "# grab a single image\n",
    "single_image = x_train[0]\n",
    "plt.imshow(single_image, 'gray') # displays the image BUT it is reversed - the digit is white and bgc is black\n",
    "plt.imshow(single_image, 'gray_r') # reverse the color mapping\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PreProcessing Data**:\n",
    "\n",
    "We first need to make sure the labels will be understandable by our CNN.\n",
    "\n",
    "**Labels**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our labels are literally categories of numbers. We need to translate this to be \"one hot encoded\" so our CNN can understand them, otherwise it will think this is some sort of regression problem on a continuous axis. Luckily , Keras has an easy to use function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical # Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "# categorical versions\n",
    "# labels; no of classes\n",
    "y_cat_test = to_categorical(y_test, 10)\n",
    "y_cat_train = to_categorical(y_train, 10)\n",
    "\n",
    "y_cat_train[0] # we know that is a 5 so we should see the 1 on the index 5 after one hot encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing X Data**:\n",
    "\n",
    "We should normalize the X data, because we have the values in range from 0 to 255 and they should be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image.max()\n",
    "# single_image.min()\n",
    "\n",
    "x_train = x_train / x_train.max() # / 255\n",
    "x_test = x_test / x_test.max() # / 255\n",
    "\n",
    "scaled_image = x_train[0]\n",
    "scaled_image.max()\n",
    "\n",
    "# we do not see any visual difference\n",
    "plt.imshow(scaled_image, 'gray_r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshaping the Data**:\n",
    "\n",
    "Right now our data is 60,000 images stored in 28 by 28 pixel array formation (60000, 28, 28). \n",
    "\n",
    "This is correct for a CNN, but we need to add one more dimension to show we're dealing with 1 RGB channel (since technically the images are in black and white, only showing values from 0-255 on a single channel), an color image would have 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to include channel dimension (in this case, 1 channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 28, 28, 1) # just defining that there is a channel there\n",
    "# we are just clarifying that we have a single channel\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "# create a model\n",
    "model = Sequential()\n",
    "\n",
    "# CONVOLUTIONAL LAYER\n",
    "# we can play arround with those values but the ones given here are usually a good starting point\n",
    "# although we can not mess around with the input shape\n",
    "model.add(Conv2D(filters=32, kernel_size=(4, 4), input_shape=(28, 28, 1), activation='relu'))\n",
    "\n",
    "# POOLING LAYER\n",
    "# we can experiment with the pool size\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "# FLATTEN IMAGES FROM 28 by 28 to 764 BEFORE FINAL LAYER (2D --> 1D)\n",
    "# we have to transform the convolutional and pooling layers into something that a single dense layer can understand\n",
    "model.add(Flatten())\n",
    "\n",
    "# DENSE HIDDEN LAYER\n",
    "# here we have 128 neurons in a hidden layer, but we can play arround with these values\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# OUTPUT LAYER\n",
    "# can not play arround with; output 10 labels and specific activation function that will directly output the class that it thinks it is\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# loss: String (name of objective function) or objective function. Configures the model for training.\n",
    "# optimizer: String (name of optimizer) or optimizer instance. Configures the model for training.\n",
    "# metrics: List of metrics to be evaluated by the model. Configures the model for training.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a string summary of the network.\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Model**:\n",
    "\n",
    "This can take a while to compute, change no of epochs if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_cat_train, epochs= 2) # have to remember that it has to be a categorical data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the Model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_cat_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us now test our model on images that is has not seen before**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# predicting on images it does not know\n",
    "predictions = model.predict_classes(x_test)\n",
    "\n",
    "# here we are not using the categorical values and not one hot encoded because the predictions have the original format\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a good performance for me."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">9. </span>Keras CNN with CIFAR-10."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with color images. This is the famous dataset with color images of various objects.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**The Data**:\n",
    "\n",
    "CIFAR-10 is a dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape # (50000, 32, 32, 3)\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0]) # frog\n",
    "plt.imshow(x_train[12]) # horse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PreProcessing**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.max() # 255\n",
    "x_train.min() # 0\n",
    "\n",
    "x_train = x_train / x_train.max()\n",
    "x_test = x_test / x_test.max()\n",
    "\n",
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels**:\n",
    "\n",
    "We can see that the labels are in their integer form, so we have to change them to one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# in this set we have 10 labels (hence the name)\n",
    "y_cat_train = to_categorical(y_train, 10)\n",
    "y_cat_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# in this model we are going to have a 2 convolutional layers set\n",
    "model.add(Conv2D(filters=32, kernel_size=(4, 4), input_shape= (32, 32, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(4, 4), input_shape= (32, 32, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# dense hidden layer\n",
    "# no of neurons is up to you but peopole tend to go for 2^n \n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# LAST LAYER IS THE CLASSIFIER, THUS 10 POSSIBLE CLASSES\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "model.fit(x_train, y_cat_train, verbose=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../cifar_10epochs.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict_classes(x_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional - Large Model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## FIRST SET OF LAYERS\n",
    "\n",
    "# CONVOLUTIONAL LAYER\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(32, 32, 3), activation='relu',))\n",
    "# CONVOLUTIONAL LAYER\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(32, 32, 3), activation='relu',))\n",
    "\n",
    "# POOLING LAYER\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "## SECOND SET OF LAYERS\n",
    "\n",
    "# CONVOLUTIONAL LAYER\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4),input_shape=(32, 32, 3), activation='relu',))\n",
    "# CONVOLUTIONAL LAYER\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4),input_shape=(32, 32, 3), activation='relu',))\n",
    "\n",
    "# POOLING LAYER\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "# FLATTEN IMAGES FROM 28 by 28 to 764 BEFORE FINAL LAYER\n",
    "model.add(Flatten())\n",
    "\n",
    "# 512 NEURONS IN DENSE HIDDEN LAYER (YOU CAN CHANGE THIS NUMBER OF NEURONS)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "# LAST LAYER IS THE CLASSIFIER, THUS 10 POSSIBLE CLASSES\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_cat_train,verbose=1,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict_classes(x_test)\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "model.save('larger_CIFAR10_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">10. </span>Deep Learning on custom images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with Custom Images**:\n",
    "\n",
    "So far everything we've worked with has been nicely formatted for us already by Keras. Let's explore what its like to work with a more realistic data set.\n",
    "\n",
    "In a real world situation, you're not going to be able to just simply upload images using a Keras import tool. Instead, you'll have to work with raw JPEG or PNG files. Now you will see how you can work for your own custom image data sets.\n",
    "\n",
    "For that we're going to need to download a pre-existing data set that has the raw image files. We're not going to be using an import tool in Keras. Instead, we'll be downloading a dataset of just raw images and we'll be learning how to resize them. Also how to perform different operations on them to make them suitable for training.\n",
    "\n",
    "And then we'll actually work with those real custom image files to build out a neural network.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**The Data**:\n",
    "\n",
    "PLEASE NOTE: THIS DATASET IS VERY LARGE. **USE OUR VERSION OF THE DATA. WE ALREADY ORGANIZED IT FOR YOU.**\n",
    "\n",
    "**ORIGINAL DATA [SOURCE](https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765).**\n",
    "\n",
    "The Kaggle Competition: [Cats and Dogs](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) includes 25,000 images of cats and dogs. We will be building a classifier that works with these images and attempt to detect dogs versus cats!\n",
    "\n",
    "The pictures are numbered 0-12499 for both cats and dogs, thus we have 12,500 images of Dogs and 12,500 images of Cats. This is a huge dataset!!\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**Note: We will be dealing with real image files, NOT numpy arrays. Which means a large part of this process will be learning how to work with and deal with large groups of image files. This is too much data to fit in memory as a numpy array, so we'll need to feed it into our model in batches.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Part one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with custom images we have to have a good file structure. Like 2 folders with names \"test\" and \"train\". Then we have to create a separate folder for each category of image (the same in both folders)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the Data**:\n",
    "\n",
    "Let's take a closer look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cat4 = cv2.imread('../Computer-Vision-with-Python/CATS_DOGS/train/CAT/4.jpg')\n",
    "cat4 = cv2.cvtColor(cat4, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "dog2 = cv2.imread('../Computer-Vision-with-Python/CATS_DOGS/train/DOG/2.jpg')\n",
    "dog2 = cv2.cvtColor(dog2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(cat4)\n",
    "plt.imshow(dog2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the images are different sizes - this is reflective of real world data.\n",
    "\n",
    "**Preparing the Data for the model**:\n",
    "\n",
    "There is too much data for us to read all at once in memory. We can use some built in functions in Keras to automatically process the data, generate a flow of batches from a directory, and also manipulate the images.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**Image Manipulation**:\n",
    "\n",
    "Its usually a good idea to manipulate the images with rotation, resizing, and scaling so the model becomes more robust to different images that our data set doesn't have. We can use the **ImageDataGenerator** to do this automatically for us. Check out the documentation for a full list of all the parameters you can use here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_gen = ImageDataGenerator(rotation_range= 30, # randomly rotate images in set range of degrees \n",
    "                               width_shift_range= 0.1,# randomly shift the pic width by a max of 10%\n",
    "                               height_shift_range=0.1, # randomly shift the pic height by a max of 10%\n",
    "                               rescale=1/255, # rescale the image by normalzing it\n",
    "                               shear_range=0.2, # shear means cutting away part of the image / crop it (max 20%)\n",
    "                               zoom_range=0.2, # zoom in by 20% max\n",
    "                               horizontal_flip=True, # allow horizontal flipping; there is also a vertical flip but we do not want to train on upside down dogs\n",
    "                               fill_mode='nearest' # fill in missing pixels with the nearest filled value; fill the gaps\n",
    "                               )\n",
    "\n",
    "# how it works?\n",
    "plt.imshow(image_gen.random_transform(dog2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating many manipulated images from a directory**:\n",
    "\n",
    "In order to use .flow_from_directory(), you must organize the images in sub-directories. This is an **absolute requirement**, otherwise the method won't work. The directories should only contain images of one class, so one folder per class of images.\n",
    "\n",
    "Structure Needed:\n",
    "\n",
    "- Image Data Folder\n",
    "    - Class 1\n",
    "        - 0.jpg\n",
    "        - 1.jpg\n",
    "        - ...\n",
    "    - Class 2\n",
    "        - 0.jpg\n",
    "        - 1.jpg\n",
    "        - ...\n",
    "    - ...\n",
    "    - Class n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to run it twice => 1 on test, 1 on train\n",
    "# here we are not doing anything with the images and we will run this later with additional parameters\n",
    "image_gen.flow_from_directory('../Computer-Vision-with-Python/CATS_DOGS/train/')\n",
    "image_gen.flow_from_directory('../Computer-Vision-with-Python/CATS_DOGS/test/')\n",
    "# Found 18743 images belonging to 2 classes.\n",
    "# Found 6251 images belonging to 2 classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Part two"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We will introduce some slightly different imports to reflect the most recent changes to Keras. These are just a few different imports:\n",
    "- MaxPool2D → MaxPooling2D\n",
    "- Adding in Activation functions separately\n",
    "\n",
    "&ensp;\n",
    "\n",
    "**Resizing Images**:\n",
    "\n",
    "Let's have Keras resize all the images to 150 pixels by 150 pixels once they've been manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width, height, channels\n",
    "image_shape = (150, 150, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "# Activation - adding the activation function after providing Dense layer\n",
    "from keras.layers import Activation, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# this is a really complicated task so we will add 3 convolutional layers\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=image_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=image_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=image_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128)) # here we will provide the activation function separately using the newly imported layer\n",
    "model.add(Activation('relu')) # works exactly the same as if we would provide it inside the Dense\n",
    "\n",
    "# DROPOUT LAYER\n",
    "# Dropouts help reduce overfitting by randomly turning neurons off during training.\n",
    "# Here we say randomly turn off 50% of neurons.\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# it is binary here => 0 = cat, 1 = dog\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Model**:\n",
    "\n",
    "We need to choose a batch size, and a good starting point is a bach size of 16. Although there is no right / correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "train_directory = '../Computer-Vision-with-Python/CATS_DOGS/train/'\n",
    "test_directory = '../Computer-Vision-with-Python/CATS_DOGS/test/'\n",
    "\n",
    "# target_size: Tuple of integers (height, width), default: (256, 256). The dimensions to which all images found will be resized.\n",
    "#class_mode: One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. Default: \"categorical\". Determines the type of label arrays that are returned\n",
    "train_image_generator = image_gen.flow_from_directory(train_directory, \n",
    "                                                      target_size=image_shape[:2],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      class_mode='binary')\n",
    "\n",
    "test_image_generator = image_gen.flow_from_directory(test_directory, \n",
    "                                                      target_size=image_shape[:2],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator.class_indices\n",
    "#{'CAT': 0, 'DOG': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the way to ignore the warning when the model can not read certain images\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps_per_epoch: Integer. Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch\n",
    "\n",
    "results = model.fit_generator(train_image_generator,epochs=1, # should be 100 but we do not have time for that. NOW we dont...;)\n",
    "                              steps_per_epoch=150, # grab a batch of 16 for 150 times and call it an epoch; more of the limiter because here we have >18k images\n",
    "                              validation_data=test_image_generator, # \n",
    "                              validation_steps=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the accuracy of the model\n",
    "results.history['acc']\n",
    "\n",
    "# we can plot the results to see how our accuracy changed through each epoch\n",
    "plt.plot(results.history['acc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting on new images**:\n",
    "\n",
    "For this we will be using a pre-trained model provided but we can do it ourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "new_model = load_model('../Computer-Vision-with-Python/06-Deep-Learning-Computer-Vision/cat_dog_100epochs.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on the image the model has never seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "dog_file = '../Computer-Vision-with-Python/CATS_DOGS/test/DOG/10005.jpg'\n",
    "\n",
    "# adjust the image to be the size we trained our model on (150, 150)\n",
    "# it is actually reading the image into PIL image format\n",
    "dog_img = image.load_img(dog_file, target_size=(150, 150))\n",
    "\n",
    "# turn the image into array because the model works on array, not PIL Image type. So we have to convert it.\n",
    "dog_img = image.img_to_array(dog_img)\n",
    "\n",
    "dog_img.shape # (150, 150, 3) BUT has to be (1, 150, 150, 3)\n",
    "# we have to expand the dimensions\n",
    "# You can use reshape. Just in this case, expand_dims easily achieve what we want here (to add another dimension to the array) for model input purpose.\n",
    "# Expand the shape of an array. Insert a new axis that will appear at the axis position in the expanded array shape.\n",
    "dog_img = np.expand_dims(dog_img, axis=0)\n",
    "\n",
    "# normalize the image\n",
    "dog_img = dog_img / dog_img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = new_model.predict_classes(dog_img) # predicts the class / label\n",
    "prediction_prob = new_model.predict(dog_img) # says how shure it is about its decision\n",
    "\n",
    "# Output prediction\n",
    "print(f'Probability that image is a dog is: {prediction_prob}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to create the confusion matrix when we use a ImageDataGenerator: [**link**](https://datascience.stackexchange.com/questions/46182/keras-confusion-matrix-with-predict-generator)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellowgreen;\">11. </span>YOLO v3 Object detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">a) </span>Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s learn about the state of the art image detection algorithm known as **YOLO (You Only Look Once)**. YOLO can view an image and draw bounding boxes over what it perceives as identified classes.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\"><div style=\"margin-top: 0.5em; margin-bottom: -0.3em; width: 25%;\">\n",
    "<img src=\"./src/img/YOLO/YOLO_1.png\">\n",
    "</div></div>\n",
    "\n",
    "We will be using version 3 of the YOLO Object Detection Algorithm, which further improves upon the original implementation in both speed and accuracy.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "So what makes YOLO different than other detection algorithms?\n",
    "- Prior detection systems repurpose classifiers or localizers to perform detection. They apply the model to an image at multiple locations and scales. High scoring regions of the image are considered detections.\n",
    "- YOLO uses a totally different approach. We apply a single neural network to the full image. This network divides the image into regions and predicts bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities.\n",
    "\n",
    "&ensp;\n",
    "\n",
    "YOLO has several advantages over classifier-based systems.\n",
    "- It looks at the whole image at test time so its predictions are informed by global context in the image.\n",
    "- It also makes predictions with a single network evaluation unlike systems like R-CNN which require thousands for a single image. This makes it extremely fast, more than 1000x faster than R-CNN and 100x faster than Fast R-CNN. \n",
    "\n",
    "We will load an already trained YOLO model and see how we can use it with either image or video data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">b) </span>Python introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now explore how to implement YOLO v3 with Python. We will be using an implementation of YOLO v3 that has been trained on the COCO dataset.\n",
    "\n",
    "The COCO dataset has over 1.5 million object instances with 80 different object categories. We will use a pre-trained model that has been trained on the COCO dataset and explore its capabilities. Realistically it would take many, many hours of training using a high end GPU to achieve a reasonable model.\n",
    "\n",
    "Because of this, we will download the weights of the pre-trained network. This network is hugely complex, meaning the actual h5 file for the weights is over 200 MB.\n",
    "\n",
    "<sup>Once you’ve downloaded the file, you will need to place it in the DATA directory of the YOLO folder.</sup>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use the state of the art in object detection. Since we can't reasonably train the YOLOv3 network ourself, we will use a pre-established version.\n",
    "\n",
    "[CODE SOURCE](https://github.com/xiaochus/YOLOv3)\n",
    "\n",
    "REFERENCE (for original YOLOv3): \n",
    "\n",
    "      @article{YOLOv3,  \n",
    "            title={YOLOv3: An Incremental Improvement},  \n",
    "            author={J Redmon, A Farhadi },\n",
    "            year={2018} \n",
    "\n",
    "**YOU MUST PROPERLY SET UP THE MODEL AND WEIGHTS. THIS NOTEBOOK WON'T WORK UNLESS YOU FOLLOW THE EXACT SET UP.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: royalblue;\">c) </span>Python implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was provided as a ready to use notebook / configuration, which can be found in the \"./Assessmests/Solutions Provided/06-YOLOv3/06-YOLO-Object-Detection.ipynb\" **BUT** it requires the pretrained model weights in the \"/06-YOLOv3/data\" directory (\"./Assessmests/Solutions Provided/06-YOLOv3/data/yolo.h5\"), which is not contained in this repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-cvcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
